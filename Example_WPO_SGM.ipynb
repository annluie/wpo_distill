{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score-matching informed KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memory_profiler in /opt/conda/lib/python3.10/site-packages (0.61.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from memory_profiler) (5.9.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install memory_profiler\n",
    "!pip install tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import lib.toy_data as toy_data\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix as pdsm\n",
    "import function_cpu as LearnCholesky\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from memory_profiler import profile\n",
    "from tqdm import trange\n",
    "import time\n",
    "import gc\n",
    "# git testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"GPU is not available\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing for scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(' ')\n",
    "parser.add_argument('--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings','swissroll_6D_xy1', 'cifar10'], type = str,default = '2spirals')\n",
    "parser.add_argument('--depth',help = 'number of hidden layers of score network',type =int, default = 5)\n",
    "parser.add_argument('--hiddenunits',help = 'number of nodes per hidden layer', type = int, default = 64)\n",
    "parser.add_argument('--niters',type = int, default = 5000)\n",
    "parser.add_argument('--batch_size', type = int,default = 4)\n",
    "parser.add_argument('--lr',type = float, default = 2e-3) \n",
    "parser.add_argument('--save',type = str,default = 'experiments/')\n",
    "parser.add_argument('--train_kernel_size',type = int, default = 100)\n",
    "parser.add_argument('--train_samples_size',type = int, default = 500)\n",
    "parser.add_argument('--test_samples_size',type = int, default = 5)\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_directory experiments/test/\n"
     ]
    }
   ],
   "source": [
    "train_kernel_size = args.train_kernel_size\n",
    "train_samples_size = args.train_samples_size\n",
    "test_samples_size = args.test_samples_size\n",
    "dataset = args.data \n",
    "save_directory = args.save + 'test'+'/'\n",
    "\n",
    "print('save_directory',save_directory)\n",
    "\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "    print('Created directory ' + save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision matrix model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cholesky factor model\n",
    "def construct_factor_model(dim:int, depth:int, hidden_units:int):\n",
    "    '''\n",
    "    Initializes neural network that models the Cholesky factor of the precision matrix # For nD examples (in theory)\n",
    "    '''\n",
    "    chain = []\n",
    "    chain.append(nn.Linear(dim,int(hidden_units),bias =True)) \n",
    "    chain.append(nn.GELU())\n",
    "\n",
    "    for _ in range(depth-1):\n",
    "        chain.append(nn.Linear(int(hidden_units),int(hidden_units),bias = True))\n",
    "        chain.append(nn.GELU())\n",
    "    chain.append(nn.Linear(int(hidden_units),int(dim*(dim+1)/2),bias = True)) \n",
    "\n",
    "    return nn.Sequential(*chain)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def evaluate_model(factornet, kernel_centers, num_test_sample):\n",
    "    '''\n",
    "    Evaluate the model by computing the average total loss over 10 batch of testing samples\n",
    "    '''\n",
    "    total_loss_sum = 0\n",
    "    device = kernel_centers.device\n",
    "    for i in range(10):\n",
    "        p_samples = toy_data.inf_train_gen(dataset,batch_size = num_test_sample)\n",
    "        testing_samples = torch.tensor(p_samples).to(dtype = torch.float32).to(device)\n",
    "        total_loss = LearnCholesky.score_implicit_matching(factornet,testing_samples,kernel_centers)\n",
    "        total_loss_sum += total_loss.item()\n",
    "         # Free up memory\n",
    "        del p_samples, testing_samples, total_loss\n",
    "        #gc.collect()\n",
    "        torch.cuda.empty_cache()  # Only if using GPU\n",
    "    average_total_loss = total_loss_sum / 10\n",
    "    return average_total_loss\n",
    "\n",
    "def save_training_slice_cov(factornet, means, epoch, lr, batch_size, loss_value, save):\n",
    "    '''\n",
    "    Save the training slice of the density plot\n",
    "    '''\n",
    "    if means.shape[1] != 2:\n",
    "        return\n",
    "    plot_axis = means.max().item() * 1.1\n",
    "    device = means.device\n",
    "    # Create x as a NumPy array\n",
    "    x_np = np.meshgrid(np.linspace(-plot_axis, plot_axis, 200), np.linspace(-plot_axis, plot_axis, 200))\n",
    "    x_np = np.stack(x_np, axis=-1).reshape(-1, 2)\n",
    "\n",
    "    x = torch.tensor(x_np, dtype=torch.float32, device=device)\n",
    "    data_dim = x.shape[1]\n",
    "    precisions = LearnCholesky.vectors_to_precision(factornet(means),data_dim)\n",
    "    density = LearnCholesky.mog_density(x, means, precisions)\n",
    "    density = density.reshape(200, 200).T\n",
    "\n",
    "    # Create a figure\n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "    plt.clf()\n",
    "    \n",
    "    plt.subplot(1, 2, 1) \n",
    "    plt.contourf(np.linspace(-plot_axis, plot_axis, 200), np.linspace(-plot_axis, plot_axis, 200), density.detach().cpu().numpy(), cmap='viridis')\n",
    "    plt.axis('square')\n",
    "    plt.colorbar()     \n",
    "    \n",
    "    plt.subplot(1, 2, 2) \n",
    "    plt.contourf(np.linspace(-plot_axis, plot_axis, 200), np.linspace(-plot_axis, plot_axis, 200), density.detach().cpu().numpy(), cmap='viridis')\n",
    "    # Plot the centers\n",
    "    num_components = torch.min(torch.tensor([means.shape[0], 400]))\n",
    "    plot_centers = means[:num_components].detach().cpu().numpy()\n",
    "    plt.scatter(plot_centers[:,1], plot_centers[:,0], s=0.2, c='r')\n",
    "    plt.axis('square')\n",
    "    # plt.colorbar()    \n",
    "    plt.title(f'Epoch: {epoch}, Loss: {loss_value:.3e}')\n",
    "             \n",
    "    plt.tight_layout()  # Improve subplot spacing\n",
    "\n",
    "    # Save the figure\n",
    "    lr_str = f'{lr:.2e}'\n",
    "    if save is not None:\n",
    "        plt.savefig(f'{save}batch_size_{batch_size}lr_{lr_str}_epoch_{epoch}.png')\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize score network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dim 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<Axes: xlabel='0', ylabel='0'>, <Axes: xlabel='1', ylabel='0'>],\n",
       "       [<Axes: xlabel='0', ylabel='1'>, <Axes: xlabel='1', ylabel='1'>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the dataset\n",
    "dataset = args.data\n",
    "dataset = 'swissroll'\n",
    "# dataset = 'swissroll_6D_xy1'\n",
    "means  = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = 1000)).to(dtype = torch.float32)\n",
    "data_dim = means.shape[1]\n",
    "print('data_dim',data_dim)\n",
    "\n",
    "blah = pd.DataFrame(means)\n",
    "pdsm(blah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Data using CIFAR-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_325981/310266888.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  means  = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = train_kernel_size)).to(dtype = torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"\" # not used anymore since our data is pictures\\nmeans  = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = 1000)).to(dtype = torch.float32)\\ndata_dim = means.shape[1]\\nprint(\\'data_dim\\',data_dim)\\n\\nblah = pd.DataFrame(means)\\npdsm(blah)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dataset\n",
    "dataset = args.data\n",
    "dataset = 'cifar10'\n",
    "means  = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = train_kernel_size)).to(dtype = torch.float32)\n",
    "data_dim = means.shape[1]\n",
    "# dataset = 'swissroll_6D_xy1'\n",
    "\"\"\"\"\" # not used anymore since our data is pictures\n",
    "means  = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = 1000)).to(dtype = torch.float32)\n",
    "data_dim = means.shape[1]\n",
    "print('data_dim',data_dim)\n",
    "\n",
    "blah = pd.DataFrame(means)\n",
    "pdsm(blah)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_325981/3990223035.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_samples = torch.tensor(p_samples).to(dtype = torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_325981/3990223035.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  centers  = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = train_kernel_size)).to(dtype = torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "depth = args.depth\n",
    "hidden_units = args.hiddenunits\n",
    "factornet = construct_factor_model(data_dim, depth, hidden_units).to(device).to(dtype = torch.float32)\n",
    "\n",
    "lr = args.lr\n",
    "optimizer = optim.Adam(factornet.parameters(), lr=args.lr)\n",
    "\n",
    "p_samples = toy_data.inf_train_gen(dataset,batch_size = train_samples_size)\n",
    "training_samples = torch.tensor(p_samples).to(dtype = torch.float32).to(device)\n",
    "centers  = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = train_kernel_size)).to(dtype = torch.float32).to(device)\n",
    "\n",
    "# torch.save(centers, save_directory + 'centers.pt')\n",
    "\n",
    "epochs = args.niters\n",
    "batch_size = args.batch_size\n",
    "\n",
    "# Training the score network\n",
    "#loss = evaluate_model(factornet, centers, test_samples_size)\n",
    "#formatted_loss = f'{loss:.3e}'  # Format the average with up to 1e-3 precision\n",
    "#print(f'Before train, Average total_loss: {formatted_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/5000 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 started\n",
      "ERROR: Could not find file /var/tmp/ipykernel_325981/1704203405.py\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   3818.8 MiB   3818.8 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   3818.8 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30   7419.1 MiB   3600.2 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31   7491.1 MiB     72.1 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32   7491.1 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  11144.2 MiB   3653.0 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  11144.2 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/wpo_distill/function_cpu.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast('cpu', dtype=torch.bfloat16):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  11144.2 MiB  11144.2 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  11144.2 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  11144.2 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  21946.8 MiB  10802.6 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  21961.8 MiB     15.0 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  21961.8 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  21961.8 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  21961.8 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  21962.7 MiB      0.9 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  21967.2 MiB      4.5 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  21967.2 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  21967.2 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  21967.2 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  21967.2 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  21967.2 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  21967.2 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  21967.2 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  21967.2 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  21967.2 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  11144.2 MiB  11144.2 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  11144.2 MiB      0.0 MiB           1       results = []\n",
      "   239  21967.2 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  11144.2 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  21967.2 MiB  10823.1 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  21967.2 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  21967.2 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  21967.2 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  21967.2 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  21967.2 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  21967.2 MiB  21967.2 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  21967.2 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  21967.2 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  21967.2 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  32779.4 MiB  10812.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  32788.9 MiB      9.5 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  32788.9 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  32788.9 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  32788.9 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  32788.9 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  32788.9 MiB      0.0 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  32793.7 MiB      4.8 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  32793.7 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  32793.7 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  32793.7 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  32793.7 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   1995.7 MiB   1995.7 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   1995.7 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   1995.7 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   3818.8 MiB   1823.1 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  11144.2 MiB   7325.3 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  32793.7 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  11144.2 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  11144.2 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  11144.2 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  21967.2 MiB  10823.1 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  32793.7 MiB  10826.4 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  32793.7 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  32793.7 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  32793.7 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  32793.7 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  32793.7 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  32793.7 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/5000 [02:08<178:15:37, 128.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss value: 5.753e+04\n",
      "Step 1 started\n",
      "ERROR: Could not find file /var/tmp/ipykernel_325981/1704203405.py\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   7600.9 MiB   7600.9 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   7600.9 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11200.9 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11273.0 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11273.0 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  14872.9 MiB   3599.9 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  14872.9 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  14872.9 MiB  14872.9 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  14872.9 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  14872.9 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  25677.8 MiB  10804.9 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  25678.0 MiB      0.2 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  25678.0 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  25678.0 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  25678.0 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  25678.0 MiB      0.0 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  25678.0 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  25678.0 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  25678.0 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  25678.0 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  25678.0 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  25678.0 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  25678.0 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  25678.0 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  25678.0 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  25678.0 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  14872.9 MiB  14872.9 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  14872.9 MiB      0.0 MiB           1       results = []\n",
      "   239  25678.0 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  14872.9 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  25678.0 MiB  10805.1 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  25678.0 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  25678.0 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  25678.0 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  25678.0 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  25678.0 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  25678.0 MiB  25678.0 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  25678.0 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  25678.0 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  25678.0 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36510.1 MiB  10832.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36510.8 MiB      0.7 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36510.8 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36510.8 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36510.8 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36510.8 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36510.8 MiB      0.0 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36515.0 MiB      4.2 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36515.0 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36515.0 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36515.0 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36515.0 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   5800.3 MiB   5800.3 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   5800.3 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   5800.3 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   7600.9 MiB   1800.6 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  14872.9 MiB   7272.0 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36515.0 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  14872.9 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  14872.9 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  14872.9 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  25678.0 MiB  10805.1 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36515.0 MiB  10837.0 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36515.0 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36515.0 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36515.0 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36515.0 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36515.0 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36515.0 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2/5000 [03:39<147:52:30, 106.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 started\n",
      "ERROR: Could not find file /var/tmp/ipykernel_325981/1704203405.py\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   7825.3 MiB   7825.3 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   7825.3 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11425.2 MiB   3599.9 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11497.2 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11497.2 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15097.2 MiB   3600.0 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15097.2 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15097.2 MiB  15097.2 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15097.2 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15097.2 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  25898.7 MiB  10801.5 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  25899.0 MiB      0.3 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  25899.0 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  25899.0 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  25899.0 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  25899.0 MiB      0.0 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  25899.0 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  25899.0 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  25899.0 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  25899.0 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  25899.0 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  25899.0 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  25899.0 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  25899.0 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  25899.0 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  25899.0 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15097.2 MiB  15097.2 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15097.2 MiB      0.0 MiB           1       results = []\n",
      "   239  25899.0 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15097.2 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  25899.0 MiB  10801.8 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  25899.0 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  25899.0 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  25899.0 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  25899.0 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  25899.0 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  25899.0 MiB  25899.0 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  25899.0 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  25899.0 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  25899.0 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36716.0 MiB  10817.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36716.4 MiB      0.4 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36716.4 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36716.4 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36716.4 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36716.4 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36716.4 MiB      0.0 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36716.4 MiB      0.0 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36716.4 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36716.4 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36716.4 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36716.4 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   6024.6 MiB   6024.6 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   6024.6 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   6024.6 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   7825.3 MiB   1800.7 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15097.2 MiB   7271.8 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36716.4 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15097.2 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15097.2 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15097.2 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  25899.0 MiB  10801.8 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36716.4 MiB  10817.5 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36716.4 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36716.4 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36716.4 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36716.4 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36716.4 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36716.4 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 3/5000 [05:09<137:21:08, 98.95s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 started\n",
      "ERROR: Could not find file /var/tmp/ipykernel_325981/1704203405.py\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   7920.1 MiB   7920.1 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   7920.1 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11520.3 MiB   3600.1 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11592.3 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11592.3 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15192.2 MiB   3600.0 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15192.2 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15192.2 MiB  15192.2 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15192.2 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15192.2 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  25992.3 MiB  10800.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  25992.5 MiB      0.2 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  25992.5 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  25992.5 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  25992.5 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  25992.5 MiB      0.0 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  25992.5 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  25992.5 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  25992.5 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  25992.5 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  25992.5 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  25992.5 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  25992.5 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  25992.5 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  25992.5 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  25992.5 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15192.2 MiB  15192.2 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15192.2 MiB      0.0 MiB           1       results = []\n",
      "   239  25992.5 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15192.2 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  25992.5 MiB  10800.3 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  25992.5 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  25992.5 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  25992.5 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  25992.5 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  25992.5 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  25992.5 MiB  25992.5 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  25992.5 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  25992.5 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  25992.5 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36811.6 MiB  10819.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36811.9 MiB      0.3 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36811.9 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36811.9 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36811.9 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36811.9 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36811.9 MiB      0.0 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36811.9 MiB      0.0 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36811.9 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36811.9 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36811.9 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36811.9 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   6119.7 MiB   6119.7 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   6119.7 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   6119.7 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   7920.1 MiB   1800.5 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15192.2 MiB   7272.1 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36811.9 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15192.2 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15192.2 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15192.2 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  25992.5 MiB  10800.3 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36811.9 MiB  10819.4 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36811.9 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36811.9 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36811.9 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36811.9 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36811.9 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36811.9 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 4/5000 [06:40<132:49:18, 95.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 started\n",
      "ERROR: Could not find file /var/tmp/ipykernel_325981/1704203405.py\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   8078.8 MiB   8078.8 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   8078.8 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11678.9 MiB   3600.1 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11750.9 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11750.9 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15351.0 MiB   3600.0 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15351.0 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15351.0 MiB  15351.0 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15351.0 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15351.0 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  26151.1 MiB  10800.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  26151.1 MiB      0.0 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  26151.1 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  26151.1 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  26151.1 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  26151.1 MiB      0.0 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  26151.1 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  26151.1 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  26151.1 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  26151.1 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  26151.1 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  26151.1 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  26151.1 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  26151.1 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  26151.1 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  26151.1 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15351.0 MiB  15351.0 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15351.0 MiB      0.0 MiB           1       results = []\n",
      "   239  26151.1 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15351.0 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  26151.1 MiB  10800.1 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  26151.1 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  26151.1 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  26151.1 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  26151.1 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  26151.1 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  26151.1 MiB  26151.1 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  26151.1 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  26151.1 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  26151.1 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36967.9 MiB  10816.8 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36968.5 MiB      0.6 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36968.5 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36968.5 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36968.5 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36968.5 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36968.5 MiB      0.0 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36968.5 MiB      0.0 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36968.5 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36968.5 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36968.5 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36968.5 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   6278.0 MiB   6278.0 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   6278.0 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   6278.0 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   8078.8 MiB   1800.8 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15351.0 MiB   7272.2 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36968.5 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15351.0 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15351.0 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15351.0 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  26151.1 MiB  10800.1 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36968.5 MiB  10817.4 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36968.5 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36968.5 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36968.5 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36968.5 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36968.5 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36968.5 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 5/5000 [08:10<129:56:38, 93.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 started\n",
      "ERROR: Could not find file /var/tmp/ipykernel_325981/1704203405.py\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   8190.4 MiB   8190.4 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   8190.4 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11790.5 MiB   3600.1 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11862.5 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11862.5 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15462.5 MiB   3600.0 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15462.5 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15462.5 MiB  15462.5 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15462.5 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15462.5 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  26262.6 MiB  10800.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  26262.6 MiB     -0.0 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  26262.6 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  26262.6 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  26262.6 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  26262.6 MiB      0.0 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  26262.6 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  26262.6 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  26262.6 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  26262.6 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  26262.6 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  26262.6 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  26262.6 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  26262.6 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  26262.6 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  26262.6 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15462.5 MiB  15462.5 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15462.5 MiB      0.0 MiB           1       results = []\n",
      "   239  26262.6 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15462.5 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  26262.6 MiB  10800.1 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  26262.6 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  26262.6 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  26262.6 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  26262.6 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  26262.6 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  26262.6 MiB  26262.6 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  26262.6 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  26262.6 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  26262.6 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  37071.6 MiB  10809.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  37071.6 MiB      0.0 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  37071.6 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  37071.6 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  37071.6 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  37071.6 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  37071.6 MiB      0.0 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  37071.6 MiB      0.0 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  37071.6 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  37071.6 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  37071.6 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  37071.6 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   6389.9 MiB   6389.9 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   6389.9 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   6389.9 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   8190.4 MiB   1800.5 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15462.5 MiB   7272.1 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  37071.6 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15462.5 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15462.5 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15462.5 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  26262.6 MiB  10800.1 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  37071.6 MiB  10809.1 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  37071.6 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  37071.6 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  37071.6 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  37071.6 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  37071.6 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  37071.6 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 6/5000 [09:40<128:30:33, 92.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 started\n",
      "ERROR: Could not find file /var/tmp/ipykernel_325981/1704203405.py\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   8319.3 MiB   8319.3 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   8319.3 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11919.4 MiB   3600.1 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11991.4 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11991.4 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15591.3 MiB   3599.9 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15591.3 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15591.3 MiB  15591.3 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15591.3 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15591.3 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  26391.4 MiB  10800.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  26391.4 MiB     -0.0 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  26391.4 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  26391.4 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  26391.4 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  26391.4 MiB      0.0 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  26391.4 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  26391.4 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  26391.4 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  26391.4 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  26391.4 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  26391.4 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  26391.4 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  26391.4 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  26391.4 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  26391.4 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15591.3 MiB  15591.3 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15591.3 MiB      0.0 MiB           1       results = []\n",
      "   239  26391.4 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15591.3 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  26391.4 MiB  10800.1 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  26391.4 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  26391.4 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  26391.4 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  26391.4 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  26391.4 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  26391.4 MiB  26391.4 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  26391.4 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  26391.4 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  26391.4 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  37203.8 MiB  10812.4 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  37204.4 MiB      0.6 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  37204.4 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  37204.4 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  37204.4 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  37204.4 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  37204.4 MiB      0.0 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  37204.4 MiB      0.0 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  37204.4 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  37204.4 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  37204.4 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  37204.4 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   6518.8 MiB   6518.8 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   6518.8 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   6518.8 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   8319.3 MiB   1800.5 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15591.3 MiB   7272.0 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  37204.4 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15591.3 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15591.3 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15591.3 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  26391.4 MiB  10800.1 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  37204.4 MiB  10813.1 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  37204.4 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  37204.4 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  37204.4 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  37204.4 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  37204.4 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  37204.4 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 7/5000 [11:10<127:10:35, 91.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7 started\n",
      "ERROR: Could not find file /var/tmp/ipykernel_325981/1704203405.py\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   8406.5 MiB   8406.5 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   8406.5 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  12006.6 MiB   3600.1 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  12078.6 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  12078.6 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15678.5 MiB   3599.9 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15678.5 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15678.5 MiB  15678.5 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15678.5 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15678.5 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  26478.8 MiB  10800.3 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  26478.5 MiB     -0.2 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  26478.5 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  26478.5 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  26478.5 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  26478.5 MiB      0.0 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  26478.5 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  26478.5 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  26478.5 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  26478.5 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  26478.5 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  26478.5 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  26478.5 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  26478.5 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  26478.5 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  26478.5 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15678.5 MiB  15678.5 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15678.5 MiB      0.0 MiB           1       results = []\n",
      "   239  26478.5 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15678.5 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  26478.5 MiB  10800.0 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  26478.5 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  26478.5 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  26478.5 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  26478.5 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  26478.5 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  26478.5 MiB  26478.5 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  26478.5 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  26478.5 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  26478.5 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  37289.4 MiB  10810.8 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  37289.5 MiB      0.1 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  37289.5 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  37289.5 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  37289.5 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  37289.5 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  37289.5 MiB      0.0 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  37289.5 MiB      0.0 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  37289.5 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  37289.5 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  37289.5 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  37289.5 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   6606.0 MiB   6606.0 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   6606.0 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   6606.0 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   8406.5 MiB   1800.5 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15678.5 MiB   7272.0 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  37289.5 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15678.5 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15678.5 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15678.5 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  26478.5 MiB  10800.0 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  37289.5 MiB  10811.0 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  37289.5 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  37289.5 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  37289.5 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  37289.5 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  37289.5 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  37289.5 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 8/5000 [12:40<126:31:35, 91.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8 started\n",
      "ERROR: Could not find file /var/tmp/ipykernel_325981/1704203405.py\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   8482.0 MiB   8482.0 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   8482.0 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  12082.1 MiB   3600.1 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  12154.1 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  12154.1 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15754.1 MiB   3600.0 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15754.1 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15754.1 MiB  15754.1 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15754.1 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15754.1 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  26554.3 MiB  10800.3 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  26554.0 MiB     -0.3 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  26554.0 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  26554.0 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  26554.0 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  26554.0 MiB      0.0 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  26554.0 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  26554.0 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  26554.0 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  26554.0 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  26554.0 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  26554.0 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  26554.0 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  26554.0 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  26554.0 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  26554.0 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15754.1 MiB  15754.1 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15754.1 MiB      0.0 MiB           1       results = []\n",
      "   239  26554.0 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15754.1 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  26554.0 MiB  10800.0 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  26554.0 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  26554.0 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  26554.0 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  26554.0 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  26554.0 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  26554.0 MiB  26554.0 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  26554.0 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  26554.0 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  26554.0 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  37364.3 MiB  10810.3 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  37364.4 MiB      0.1 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  37364.4 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  37364.4 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  37364.4 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  37364.4 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  37364.4 MiB      0.0 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  37364.4 MiB      0.0 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  37364.4 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  37364.4 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  37364.4 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  37364.4 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   6681.5 MiB   6681.5 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   6681.5 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   6681.5 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   8482.0 MiB   1800.5 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15754.1 MiB   7272.1 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  37364.4 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15754.1 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15754.1 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15754.1 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  26554.0 MiB  10800.0 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  37364.4 MiB  10810.4 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  37364.4 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  37364.4 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  37364.4 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  37364.4 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  37364.4 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  37364.4 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 8/5000 [14:10<147:20:35, 106.26s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m randind \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, train_samples_size, [batch_size,])\n\u001b[1;32m     13\u001b[0m samples \u001b[38;5;241m=\u001b[39m training_samples[randind, :]\n\u001b[0;32m---> 14\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mopt_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactornet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m4000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:1188\u001b[0m, in \u001b[0;36mprofile.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(wrapped\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1187\u001b[0m     prof \u001b[38;5;241m=\u001b[39m get_prof()\n\u001b[0;32m-> 1188\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mprof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     show_results_bound(prof)\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:761\u001b[0m, in \u001b[0;36mLineProfiler.wrap_function.<locals>.f\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_ctxmgr():\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m, in \u001b[0;36mopt_check\u001b[0;34m(factornet, samples, centers)\u001b[0m\n\u001b[1;32m      3\u001b[0m loss \u001b[38;5;241m=\u001b[39m LearnCholesky\u001b[38;5;241m.\u001b[39mscore_implicit_matching(factornet,samples,centers)\n\u001b[1;32m      4\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:292\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[1;32m    288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:791\u001b[0m, in \u001b[0;36mLineProfiler.trace_memory_usage\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    789\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrace_memory_usage\u001b[39m(\u001b[38;5;28mself\u001b[39m, frame, event, arg):\n\u001b[1;32m    792\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Callback for sys.settrace\"\"\"\u001b[39;00m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frame\u001b[38;5;241m.\u001b[39mf_code \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcode_map:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@profile\n",
    "def opt_check(factornet, samples, centers):\n",
    "    loss = LearnCholesky.score_implicit_matching(factornet,samples,centers)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Use trange instead of range to get tqdm progress bar\n",
    "for step in trange(epochs, desc=\"Training\"):\n",
    "    print(f\"Step {step} started\")\n",
    "    randind = torch.randint(0, train_samples_size, [batch_size,])\n",
    "    samples = training_samples[randind, :]\n",
    "    loss_value = opt_check(factornet, samples, centers)\n",
    "\n",
    "    if step % 4000 == 0:\n",
    "        print(f'Step: {step}, Loss value: {loss_value:.3e}')\n",
    "    \"\"\"\n",
    "    if step % 20000 == 0:\n",
    "        loss0 = evaluate_model(factornet, centers, test_samples_size)\n",
    "        save_training_slice_cov(factornet, centers, step, lr, batch_size, loss0, save_directory)\n",
    "    \"\"\"\n",
    "    \n",
    "    if step < epochs - 1:\n",
    "        del samples\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file /var/tmp/ipykernel_174720/1267574142.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   3825.4 MiB   3825.4 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   3825.4 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30   7425.4 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31   7497.4 MiB     72.1 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32   7497.4 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  11152.4 MiB   3655.0 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  11152.4 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/wpo_distill/function_cpu.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast('cpu', dtype=torch.bfloat16):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  11152.4 MiB  11152.4 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  11152.4 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  11152.4 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  21955.3 MiB  10802.9 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  21969.8 MiB     14.5 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  21969.8 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  21969.8 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  21969.8 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  21970.6 MiB      0.8 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  21975.2 MiB      4.5 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  21975.2 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  21975.2 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  21975.2 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  21975.2 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  21975.2 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  21975.2 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  21975.4 MiB      0.2 MiB           1       gc.collect()\n",
      "   230  21975.4 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  21975.4 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  11152.4 MiB  11152.4 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  11152.4 MiB      0.0 MiB           1       results = []\n",
      "   239  21975.4 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  11152.4 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  21975.4 MiB  10823.0 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  21975.4 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  21975.4 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  21975.4 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  21975.4 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  21975.4 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  21975.4 MiB  21975.4 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  21975.4 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  21975.4 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  21975.4 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  32787.4 MiB  10812.0 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  32796.7 MiB      9.4 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  32796.7 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  32796.7 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  32796.7 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  32796.7 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  32796.7 MiB      0.0 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  32801.0 MiB      4.3 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  32801.0 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  32801.0 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  32801.0 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  32801.0 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   2002.3 MiB   2002.3 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   2002.3 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   2002.3 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   3825.4 MiB   1823.1 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  11152.4 MiB   7327.0 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  32801.0 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  11152.4 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  11152.4 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  11152.4 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  21975.4 MiB  10823.0 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  32801.0 MiB  10825.7 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  32801.0 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  32801.0 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  32801.0 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  32801.0 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  32801.0 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  32801.0 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "Step: 0, Loss value: 4.109e+04\n",
      "ERROR: Could not find file /var/tmp/ipykernel_174720/4110140829.py\n",
      "tensor(-1.) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_174720/4110140829.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  testing_samples = torch.tensor(p_samples).to(dtype = torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   7563.0 MiB   7563.0 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   7563.0 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11163.0 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11235.0 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11235.0 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  14834.9 MiB   3599.9 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  14834.9 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  14834.9 MiB  14834.9 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  14834.9 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  14834.9 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  25634.9 MiB  10800.0 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  25640.5 MiB      5.6 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  25640.5 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  25640.5 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  25646.4 MiB      5.9 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  25652.3 MiB      5.9 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  25658.3 MiB      6.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  25658.3 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  25658.3 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  25658.3 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  25658.3 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  25658.3 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  25658.3 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  25658.5 MiB      0.2 MiB           1       gc.collect()\n",
      "   230  25658.5 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  25658.5 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  14834.9 MiB  14834.9 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  14834.9 MiB      0.0 MiB           1       results = []\n",
      "   239  25658.5 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  14834.9 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  25658.5 MiB  10823.7 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  25658.5 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  25658.5 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  25658.5 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  25658.5 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  25658.5 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  25658.5 MiB  25658.5 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  25658.5 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  25658.5 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  25658.5 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36477.9 MiB  10819.3 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36495.3 MiB     17.4 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36495.3 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36495.3 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36501.3 MiB      6.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36501.3 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36507.4 MiB      6.1 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36513.2 MiB      5.8 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36513.2 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36513.2 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36513.2 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36513.2 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   5762.3 MiB   5762.3 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   5762.3 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   5762.3 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   7563.0 MiB   1800.7 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  14834.9 MiB   7271.9 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36513.2 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  14834.9 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  14834.9 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  14834.9 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  25658.5 MiB  10823.7 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36513.2 MiB  10854.7 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36513.2 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36513.2 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36513.2 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36513.2 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36513.2 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36513.2 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-1.) tensor(1.)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   7649.0 MiB   7649.0 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   7649.0 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11249.0 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11321.0 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11321.0 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  14960.6 MiB   3639.5 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  14960.6 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  14960.6 MiB  14960.6 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  14960.6 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  14960.6 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  25760.6 MiB  10800.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  25760.2 MiB     -0.5 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  25760.2 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  25760.2 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  25760.2 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  25760.3 MiB      0.1 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  25760.3 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  25760.3 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  25760.3 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  25760.3 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  25760.3 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  25760.3 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  25760.3 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  25760.3 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  25760.3 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  25760.3 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  14960.6 MiB  14960.6 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  14960.6 MiB      0.0 MiB           1       results = []\n",
      "   239  25760.3 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  14960.6 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  25760.3 MiB  10799.8 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  25760.3 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  25760.3 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  25760.3 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  25760.3 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  25760.3 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  25760.3 MiB  25760.3 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  25760.3 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  25760.3 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  25760.3 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36579.1 MiB  10818.8 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36579.9 MiB      0.8 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36579.9 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36579.9 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36579.9 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36579.9 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36580.1 MiB      0.1 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36580.1 MiB      0.1 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36580.1 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36580.1 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36580.1 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36580.1 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   5835.0 MiB   5835.0 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   5835.0 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   5835.0 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   7649.0 MiB   1814.0 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  14960.6 MiB   7311.6 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36580.1 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  14960.6 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  14960.6 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  14960.6 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  25760.3 MiB  10799.8 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36580.1 MiB  10819.8 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36580.1 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36580.1 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36580.1 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36580.1 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36580.1 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36580.1 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-1.) tensor(1.)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   7708.2 MiB   7708.2 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   7708.2 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11308.2 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11380.2 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11380.2 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15017.3 MiB   3637.1 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15017.3 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15017.3 MiB  15017.3 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15017.3 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15017.3 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  25817.4 MiB  10800.0 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  25817.3 MiB     -0.1 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  25817.3 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  25817.3 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  25817.3 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  25817.4 MiB      0.1 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  25817.4 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  25817.4 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  25817.4 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  25817.4 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  25817.4 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  25817.4 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  25817.4 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  25817.4 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  25817.4 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  25817.4 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15017.3 MiB  15017.3 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15017.3 MiB      0.0 MiB           1       results = []\n",
      "   239  25817.4 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15017.3 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  25817.4 MiB  10800.1 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  25817.4 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  25817.4 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  25817.4 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  25817.4 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  25817.4 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  25817.4 MiB  25817.4 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  25817.4 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  25817.4 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  25817.4 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36636.3 MiB  10818.9 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36637.0 MiB      0.7 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36637.0 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36637.0 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36637.0 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36637.0 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36637.0 MiB      0.0 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36637.1 MiB      0.1 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36637.1 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36637.1 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36637.1 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36637.1 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   5896.9 MiB   5896.9 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   5896.9 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   5896.9 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   7708.2 MiB   1811.3 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15017.3 MiB   7309.1 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36637.1 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15017.3 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15017.3 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15017.3 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  25817.4 MiB  10800.1 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36637.1 MiB  10819.7 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36637.1 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36637.1 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36637.1 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36637.1 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36637.1 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36637.1 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-1.) tensor(1.)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   7758.9 MiB   7758.9 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   7758.9 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11358.9 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11430.9 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11430.9 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15067.7 MiB   3636.8 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15067.7 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15067.7 MiB  15067.7 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15067.7 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15067.7 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  25869.4 MiB  10801.7 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  25869.2 MiB     -0.2 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  25869.2 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  25869.2 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  25869.2 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  25869.3 MiB      0.1 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  25869.3 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  25869.3 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  25869.3 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  25869.3 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  25869.3 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  25869.3 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  25869.3 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  25869.3 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  25869.3 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  25869.3 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15067.7 MiB  15067.7 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15067.7 MiB      0.0 MiB           1       results = []\n",
      "   239  25869.3 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15067.7 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  25869.3 MiB  10801.6 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  25869.3 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  25869.3 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  25869.3 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  25869.3 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  25869.3 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  25869.3 MiB  25869.3 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  25869.3 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  25869.3 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  25869.3 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36676.4 MiB  10807.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36681.6 MiB      5.2 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36681.6 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36681.6 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36681.6 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36681.6 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36687.9 MiB      6.3 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36693.4 MiB      5.5 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36693.4 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36693.4 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36693.4 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36693.4 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   5958.0 MiB   5958.0 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   5958.0 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   5958.0 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   7758.9 MiB   1800.9 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15067.7 MiB   7308.8 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36693.4 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15067.7 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15067.7 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15067.7 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  25869.3 MiB  10801.6 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36693.4 MiB  10824.1 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36693.4 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36693.4 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36693.4 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36693.4 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36693.4 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36693.4 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-0.9451) tensor(0.9843)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   7800.1 MiB   7800.1 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   7800.1 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11400.1 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11472.1 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11472.1 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15111.7 MiB   3639.6 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15111.7 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15111.7 MiB  15111.7 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15111.7 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15111.7 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  25911.7 MiB  10800.0 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  25911.7 MiB     -0.0 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  25911.7 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  25911.7 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  25911.7 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  25911.8 MiB      0.1 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  25917.1 MiB      5.3 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  25917.1 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  25917.1 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  25917.1 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  25917.1 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  25917.1 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  25917.1 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  25917.3 MiB      0.2 MiB           1       gc.collect()\n",
      "   230  25917.3 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  25917.3 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15111.7 MiB  15111.7 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15111.7 MiB      0.0 MiB           1       results = []\n",
      "   239  25917.3 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15111.7 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  25917.3 MiB  10805.7 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  25917.3 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  25917.3 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  25917.3 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  25917.3 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  25917.3 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  25917.3 MiB  25917.3 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  25917.3 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  25917.3 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  25917.3 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36739.4 MiB  10822.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36756.8 MiB     17.4 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36756.8 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36756.8 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36762.9 MiB      6.1 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36762.9 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36768.9 MiB      6.1 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36774.8 MiB      5.8 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36774.8 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36774.8 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36774.8 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36774.8 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   5985.9 MiB   5985.9 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   5985.9 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   5985.9 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   7800.1 MiB   1814.2 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15111.7 MiB   7311.6 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36774.8 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15111.7 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15111.7 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15111.7 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  25917.3 MiB  10805.7 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36774.8 MiB  10857.4 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36774.8 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36774.8 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36774.8 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36774.8 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36774.8 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36774.8 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-1.) tensor(1.)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   7910.5 MiB   7910.5 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   7910.5 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11510.5 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11582.5 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11582.5 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15223.5 MiB   3640.9 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15223.5 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15223.5 MiB  15223.5 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15223.5 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15223.5 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  26025.3 MiB  10801.8 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  26024.9 MiB     -0.4 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  26024.9 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  26024.9 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  26024.9 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  26025.1 MiB      0.1 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  26025.1 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  26025.1 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  26025.1 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  26025.1 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  26025.1 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  26025.1 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  26025.1 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  26025.1 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  26025.1 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  26025.1 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15223.5 MiB  15223.5 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15223.5 MiB      0.0 MiB           1       results = []\n",
      "   239  26025.1 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15223.5 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  26025.1 MiB  10801.6 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  26025.1 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  26025.1 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  26025.1 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  26025.1 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  26025.1 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  26025.1 MiB  26025.1 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  26025.1 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  26025.1 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  26025.1 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36835.1 MiB  10810.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36835.2 MiB      0.0 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36835.2 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36835.2 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36835.2 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36835.2 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36835.4 MiB      0.2 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36835.4 MiB      0.0 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36835.4 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36835.4 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36835.4 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36835.4 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   6096.5 MiB   6096.5 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   6096.5 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   6096.5 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   7910.5 MiB   1814.0 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15223.5 MiB   7313.0 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36835.4 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15223.5 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15223.5 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15223.5 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  26025.1 MiB  10801.6 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36835.4 MiB  10810.3 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36835.4 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36835.4 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36835.4 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36835.4 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36835.4 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36835.4 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-1.) tensor(0.9922)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   7955.4 MiB   7955.4 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   7955.4 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11555.4 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11627.5 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11627.5 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15260.3 MiB   3632.8 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15260.3 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15260.3 MiB  15260.3 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15260.3 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15260.3 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  26062.4 MiB  10802.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  26062.1 MiB     -0.2 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  26062.1 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  26062.1 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  26062.1 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  26062.5 MiB      0.3 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  26062.5 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  26062.5 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  26062.5 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  26062.5 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  26062.5 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  26062.5 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  26062.5 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  26062.5 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  26062.5 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  26062.5 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15260.3 MiB  15260.3 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15260.3 MiB      0.0 MiB           1       results = []\n",
      "   239  26062.5 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15260.3 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  26062.5 MiB  10802.2 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  26062.5 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  26062.5 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  26062.5 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  26062.5 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  26062.5 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  26062.5 MiB  26062.5 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  26062.5 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  26062.5 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  26062.5 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36867.4 MiB  10804.9 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36867.6 MiB      0.2 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36867.6 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36867.6 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36867.6 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36867.6 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36867.7 MiB      0.2 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36873.1 MiB      5.4 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36873.1 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36873.1 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36873.1 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36873.1 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   6152.0 MiB   6152.0 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   6152.0 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   6152.0 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   7955.4 MiB   1803.4 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15260.3 MiB   7304.9 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36873.1 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15260.3 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15260.3 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15260.3 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  26062.5 MiB  10802.2 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36873.1 MiB  10810.6 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36873.1 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36873.1 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36873.1 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36873.1 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36873.1 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36873.1 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-1.) tensor(1.)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   8003.9 MiB   8003.9 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   8003.9 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11604.0 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11676.0 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11677.9 MiB      1.9 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15296.7 MiB   3618.8 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15296.7 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15296.7 MiB  15296.7 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15296.7 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15296.7 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  26098.8 MiB  10802.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  26098.5 MiB     -0.3 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  26098.5 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  26098.5 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  26098.5 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  26098.6 MiB      0.1 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  26098.6 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  26098.6 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  26098.6 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  26098.6 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  26098.6 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  26098.6 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  26098.6 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  26098.6 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  26098.6 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  26098.6 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15296.7 MiB  15296.7 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15296.7 MiB      0.0 MiB           1       results = []\n",
      "   239  26098.6 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15296.7 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  26098.6 MiB  10802.0 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  26098.6 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  26098.6 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  26098.6 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  26098.6 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  26098.6 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  26098.6 MiB  26098.6 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  26098.6 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  26098.6 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  26098.6 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36914.1 MiB  10815.5 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36914.3 MiB      0.2 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36914.3 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36914.3 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36914.3 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36914.3 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36914.3 MiB      0.0 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36914.3 MiB      0.0 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36914.3 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36914.3 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36914.3 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36914.3 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   6200.8 MiB   6200.8 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   6200.8 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   6200.8 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   8003.9 MiB   1803.1 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15296.7 MiB   7292.7 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36914.3 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15296.7 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15296.7 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15296.7 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  26098.6 MiB  10802.0 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36914.3 MiB  10815.7 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36914.3 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36914.3 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36914.3 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36914.3 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36914.3 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36914.3 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-0.9843) tensor(0.8510)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   8048.4 MiB   8048.4 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   8048.4 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11650.3 MiB   3601.9 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11722.3 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11722.3 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15362.4 MiB   3640.1 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15362.4 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15362.4 MiB  15362.4 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15362.4 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15362.4 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  26164.4 MiB  10802.0 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  26164.4 MiB      0.1 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  26164.4 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  26164.4 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  26164.5 MiB      0.1 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  26164.7 MiB      0.1 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  26164.7 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  26164.7 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  26164.7 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  26164.7 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  26164.7 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  26164.7 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  26164.7 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  26164.7 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  26164.7 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  26164.7 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15362.4 MiB  15362.4 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15362.4 MiB      0.0 MiB           1       results = []\n",
      "   239  26164.7 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15362.4 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  26164.7 MiB  10802.3 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  26164.7 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  26164.7 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  26164.7 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  26164.7 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  26164.7 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  26164.7 MiB  26164.7 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  26164.7 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  26164.7 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  26164.7 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36985.0 MiB  10820.3 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36986.0 MiB      1.0 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36986.0 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36986.0 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36986.0 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36986.0 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36986.2 MiB      0.2 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36991.3 MiB      5.1 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36991.3 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36991.3 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36991.3 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36991.3 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   6241.1 MiB   6241.1 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   6241.1 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   6241.1 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   8048.4 MiB   1807.3 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15362.4 MiB   7314.0 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36991.3 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15362.4 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15362.4 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15362.4 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  26164.7 MiB  10802.3 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36991.3 MiB  10826.7 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36991.3 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36991.3 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36991.3 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36991.3 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36991.3 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36991.3 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-1.) tensor(1.)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   8093.8 MiB   8093.8 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   8093.8 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11693.8 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11765.8 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11765.8 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15407.0 MiB   3641.2 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15407.0 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20000\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     loss0 \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactornet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_samples_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     save_training_slice_cov(factornet, centers, step, lr, batch_size, loss0, save_directory)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m<\u001b[39m epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     31\u001b[0m      \u001b[38;5;66;03m# Free up memory\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:1188\u001b[0m, in \u001b[0;36mprofile.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(wrapped\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1187\u001b[0m     prof \u001b[38;5;241m=\u001b[39m get_prof()\n\u001b[0;32m-> 1188\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mprof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     show_results_bound(prof)\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:761\u001b[0m, in \u001b[0;36mLineProfiler.wrap_function.<locals>.f\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_ctxmgr():\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(factornet, kernel_centers, num_test_sample)\u001b[0m\n\u001b[1;32m      9\u001b[0m p_samples \u001b[38;5;241m=\u001b[39m toy_data\u001b[38;5;241m.\u001b[39minf_train_gen(dataset,batch_size \u001b[38;5;241m=\u001b[39m num_test_sample)\n\u001b[1;32m     10\u001b[0m testing_samples \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(p_samples)\u001b[38;5;241m.\u001b[39mto(dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[43mLearnCholesky\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_implicit_matching\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactornet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtesting_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkernel_centers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m total_loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     13\u001b[0m  \u001b[38;5;66;03m# Free up memory\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:1188\u001b[0m, in \u001b[0;36mprofile.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(wrapped\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1187\u001b[0m     prof \u001b[38;5;241m=\u001b[39m get_prof()\n\u001b[0;32m-> 1188\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mprof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     show_results_bound(prof)\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:761\u001b[0m, in \u001b[0;36mLineProfiler.wrap_function.<locals>.f\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_ctxmgr():\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wpo_distill/function_cpu.py:263\u001b[0m, in \u001b[0;36mscore_implicit_matching\u001b[0;34m(factornet, samples, centers)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcenters requires grad: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcenters\u001b[38;5;241m.\u001b[39mrequires_grad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m#laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m laplacian_over_density \u001b[38;5;241m=\u001b[39m \u001b[43mlaplacian_mog_density_div_density_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprecisions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m gradient_eval_log \u001b[38;5;241m=\u001b[39m grad_log_mog_density(samples,centers,precisions)\n\u001b[1;32m    265\u001b[0m gradient_eval_log_squared \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(gradient_eval_log \u001b[38;5;241m*\u001b[39m gradient_eval_log, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:1188\u001b[0m, in \u001b[0;36mprofile.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(wrapped\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1187\u001b[0m     prof \u001b[38;5;241m=\u001b[39m get_prof()\n\u001b[0;32m-> 1188\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mprof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     show_results_bound(prof)\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:761\u001b[0m, in \u001b[0;36mLineProfiler.wrap_function.<locals>.f\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_ctxmgr():\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wpo_distill/function_cpu.py:241\u001b[0m, in \u001b[0;36mlaplacian_mog_density_div_density_chunked\u001b[0;34m(x, means, precisions, chunk_size)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), chunk_size):\n\u001b[1;32m    240\u001b[0m     x_chunk \u001b[38;5;241m=\u001b[39m x[i:i\u001b[38;5;241m+\u001b[39mchunk_size]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mrequires_grad_(x\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[0;32m--> 241\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlaplacian_mog_density_div_density\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecisions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;28;01melse\u001b[39;00m result)\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m result, x_chunk\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:1188\u001b[0m, in \u001b[0;36mprofile.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(wrapped\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1187\u001b[0m     prof \u001b[38;5;241m=\u001b[39m get_prof()\n\u001b[0;32m-> 1188\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mprof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     show_results_bound(prof)\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:761\u001b[0m, in \u001b[0;36mLineProfiler.wrap_function.<locals>.f\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_ctxmgr():\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wpo_distill/function_cpu.py:194\u001b[0m, in \u001b[0;36mlaplacian_mog_density_div_density\u001b[0;34m(x, means, precisions)\u001b[0m\n\u001b[1;32m    191\u001b[0m batch_size, num_components \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), means\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Create a batch of Multivariate Normal distributions for each component\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m mvns \u001b[38;5;241m=\u001b[39m \u001b[43mMultivariateNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecisions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each component\u001b[39;00m\n\u001b[1;32m    197\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m mvns\u001b[38;5;241m.\u001b[39mlog_prob(x)  \u001b[38;5;66;03m# Shape: (batch_size, num_components)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/distributions/multivariate_normal.py:189\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbroadcasted_scale_tril \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky(covariance_matrix)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# precision_matrix is not None\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbroadcasted_scale_tril \u001b[38;5;241m=\u001b[39m \u001b[43m_precision_to_scale_tril\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecision_matrix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/distributions/multivariate_normal.py:81\u001b[0m, in \u001b[0;36m_precision_to_scale_tril\u001b[0;34m(P)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_precision_to_scale_tril\u001b[39m(P):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Ref: https://nbviewer.jupyter.org/gist/fehiepsi/5ef8e09e61604f10607380467eb82006#Precision-to-scale_tril\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     Lf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     82\u001b[0m     L_inv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(torch\u001b[38;5;241m.\u001b[39mflip(Lf, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     83\u001b[0m     Id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(P\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mP\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mP\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "for step in range(epochs):\n",
    "    start = time.time()\n",
    "    # samples_toydata\n",
    "    randind = torch.randint(0,train_samples_size,[batch_size,])\n",
    "    samples = training_samples[randind,:]\n",
    "    loss_value = opt_check(factornet, samples, centers)\n",
    "    \"\"\"\n",
    "    loss = LearnCholesky.score_implicit_matching(factornet,samples,centers)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "    if not step % 4000:\n",
    "        #loss_value = loss.item()\n",
    "        print(f'Step: {step}, Loss value: {loss_value:.3e}')\n",
    "\n",
    "    if not step % 20000:\n",
    "        loss0 = evaluate_model(factornet, centers, test_samples_size)\n",
    "        save_training_slice_cov(factornet, centers, step, lr, batch_size, loss0, save_directory)\n",
    "    if step < epochs - 1:\n",
    "         # Free up memory\n",
    "        del samples # del loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Elapsed time:\", time.time() - start)\n",
    "    \n",
    "    \n",
    "formatted_loss = f'{loss:.3e}'  # Format the average with up to 1e-3 precision\n",
    "print(f'After train, Average total_loss: {formatted_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from trained model\n",
    "# and plot density\n",
    "precisions = LearnCholesky.vectors_to_precision(factornet(centers),data_dim)\n",
    "LearnCholesky.plot_images(centers, precisions,plot_number=10,save_path=save_directory + 'samples.png')\n",
    "\n",
    "\"\"\" I think this will not work for general (centers needs to be same as before to plot properly)\n",
    "randind = torch.randint(0,1000,[1000,])\n",
    "centers = means[randind,:].to(device)\n",
    "precisions = LearnCholesky.vectors_to_precision(factornet(centers),data_dim)\n",
    "\n",
    "LearnCholesky.scatter_samples_from_model(centers, precisions, dim1 = 0, dim2 = 1,save_path=save_directory + 'samples.png')\n",
    "LearnCholesky.plot_density_2d_marg(centers,factornet,dim1 = 0, dim2 = 1, save_path=save_directory + 'density.png')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LearnCholesky.scatter_samples_from_model(centers, precisions, dim1 = 2, dim2 = 3, save_path=save_directory + 'samples.png')\n",
    "# LearnCholesky.plot_density_2d_marg(centers,factornet, dim1 = 2, dim2 = 3, save_path=save_directory + 'density.png')\n",
    "# LearnCholesky.scatter_samples_from_model(centers, precisions, dim1 = 4, dim2 = 5,  save_path=save_directory + 'samples.png')\n",
    "# LearnCholesky.plot_density_2d_marg(centers,factornet, dim1 = 4, dim2 = 5,save_path=save_directory + 'density.png')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
