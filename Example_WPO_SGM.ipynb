{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score-matching informed KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memory_profiler in /opt/conda/lib/python3.10/site-packages (0.61.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from memory_profiler) (5.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install memory_profiler\n",
    "%pip install tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import lib.toy_data as toy_data\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix as pdsm\n",
    "import function_cpu as LearnCholesky\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from memory_profiler import profile\n",
    "from tqdm import trange\n",
    "import time\n",
    "import gc\n",
    "# git testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"GPU is not available\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing for scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(' ')\n",
    "parser.add_argument('--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings','swissroll_6D_xy1', 'cifar10'], type = str,default = '2spirals')\n",
    "parser.add_argument('--depth',help = 'number of hidden layers of score network',type =int, default = 5)\n",
    "parser.add_argument('--hiddenunits',help = 'number of nodes per hidden layer', type = int, default = 64)\n",
    "parser.add_argument('--niters',type = int, default = 50)\n",
    "parser.add_argument('--batch_size', type = int,default = 4)\n",
    "parser.add_argument('--lr',type = float, default = 2e-3) \n",
    "parser.add_argument('--save',type = str,default = 'cifar10_experiments/')\n",
    "parser.add_argument('--train_kernel_size',type = int, default = 10)\n",
    "parser.add_argument('--train_samples_size',type = int, default = 500)\n",
    "parser.add_argument('--test_samples_size',type = int, default = 5)\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_directory cifar10_experiments/test/\n"
     ]
    }
   ],
   "source": [
    "train_kernel_size = args.train_kernel_size\n",
    "train_samples_size = args.train_samples_size\n",
    "test_samples_size = args.test_samples_size\n",
    "dataset = args.data \n",
    "save_directory = args.save + 'test'+'/'\n",
    "\n",
    "print('save_directory',save_directory)\n",
    "\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "    print('Created directory ' + save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision matrix model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cholesky factor model\n",
    "def construct_factor_model(dim:int, depth:int, hidden_units:int):\n",
    "    '''\n",
    "    Initializes neural network that models the Cholesky factor of the precision matrix # For nD examples (in theory)\n",
    "    '''\n",
    "    chain = []\n",
    "    chain.append(nn.Linear(dim,int(hidden_units),bias =True)) \n",
    "    chain.append(nn.GELU())\n",
    "\n",
    "    for _ in range(depth-1):\n",
    "        chain.append(nn.Linear(int(hidden_units),int(hidden_units),bias = True))\n",
    "        chain.append(nn.GELU())\n",
    "    chain.append(nn.Linear(int(hidden_units),int(dim*(dim+1)/2),bias = True)) \n",
    "\n",
    "    return nn.Sequential(*chain)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def evaluate_model(factornet, kernel_centers, num_test_sample):\n",
    "    '''\n",
    "    Evaluate the model by computing the average total loss over 10 batch of testing samples\n",
    "    '''\n",
    "    total_loss_sum = 0\n",
    "    device = kernel_centers.device\n",
    "    for i in range(10):\n",
    "        p_samples = toy_data.inf_train_gen(dataset,batch_size = num_test_sample)\n",
    "        testing_samples = torch.tensor(p_samples).to(dtype = torch.float32).to(device)\n",
    "        total_loss = LearnCholesky.score_implicit_matching(factornet,testing_samples,kernel_centers)\n",
    "        total_loss_sum += total_loss.item()\n",
    "         # Free up memory\n",
    "        del p_samples, testing_samples, total_loss\n",
    "        #gc.collect() #only if using CPU\n",
    "        torch.cuda.empty_cache()  # Only if using GPU\n",
    "    average_total_loss = total_loss_sum / 10\n",
    "    return average_total_loss\n",
    "\n",
    "def save_training_slice_cov(factornet, means, epoch, lr, batch_size, loss_value, save):\n",
    "    '''\n",
    "    Save the training slice of the NN\n",
    "    '''\n",
    "    if save is not None:\n",
    "        torch.save(factornet.state_dict(), save + 'epoch' + str(epoch) + 'model_weights.pth')\n",
    "\n",
    "    \"\"\" OLD CODE\n",
    "    if means.shape[1] != 2:\n",
    "        return\n",
    "    plot_axis = means.max().item() * 1.1\n",
    "    device = means.device\n",
    "    # Create x as a NumPy array\n",
    "    x_np = np.meshgrid(np.linspace(-plot_axis, plot_axis, 200), np.linspace(-plot_axis, plot_axis, 200))\n",
    "    x_np = np.stack(x_np, axis=-1).reshape(-1, 2)\n",
    "\n",
    "    x = torch.tensor(x_np, dtype=torch.float32, device=device)\n",
    "    data_dim = x.shape[1]\n",
    "    precisions = LearnCholesky.vectors_to_precision(factornet(means),data_dim)\n",
    "    density = LearnCholesky.mog_density(x, means, precisions)\n",
    "\n",
    "    density = density.reshape(200, 200).T\n",
    "\n",
    "    # Create a figure\n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "    plt.clf()\n",
    "    \n",
    "    plt.subplot(1, 2, 1) \n",
    "    plt.contourf(np.linspace(-plot_axis, plot_axis, 200), np.linspace(-plot_axis, plot_axis, 200), density.detach().cpu().numpy(), cmap='viridis')\n",
    "    plt.axis('square')\n",
    "    plt.colorbar()     \n",
    "    \n",
    "    plt.subplot(1, 2, 2) \n",
    "    plt.contourf(np.linspace(-plot_axis, plot_axis, 200), np.linspace(-plot_axis, plot_axis, 200), density.detach().cpu().numpy(), cmap='viridis')\n",
    "    # Plot the centers\n",
    "    num_components = torch.min(torch.tensor([means.shape[0], 400]))\n",
    "    plot_centers = means[:num_components].detach().cpu().numpy()\n",
    "    plt.scatter(plot_centers[:,1], plot_centers[:,0], s=0.2, c='r')\n",
    "    plt.axis('square')\n",
    "    # plt.colorbar()    \n",
    "    plt.title(f'Epoch: {epoch}, Loss: {loss_value:.3e}')\n",
    "             \n",
    "    plt.tight_layout()  # Improve subplot spacing\n",
    "\n",
    "    # Save the figure\n",
    "    lr_str = f'{lr:.2e}'\n",
    "    if save is not None:\n",
    "        plt.savefig(f'{save}batch_size_{batch_size}lr_{lr_str}_epoch_{epoch}.png')\n",
    "\n",
    "    plt.close(fig)\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize score network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dim 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<Axes: xlabel='0', ylabel='0'>, <Axes: xlabel='1', ylabel='0'>],\n",
       "       [<Axes: xlabel='0', ylabel='1'>, <Axes: xlabel='1', ylabel='1'>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the dataset\n",
    "dataset = args.data\n",
    "dataset = 'swissroll'\n",
    "# dataset = 'swissroll_6D_xy1'\n",
    "means  = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = 1000)).to(dtype = torch.float32)\n",
    "data_dim = means.shape[1]\n",
    "print('data_dim',data_dim)\n",
    "\n",
    "blah = pd.DataFrame(means)\n",
    "pdsm(blah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Data using CIFAR-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_447013/310266888.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  means  = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = train_kernel_size)).to(dtype = torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"\" # not used anymore since our data is pictures\\nmeans  = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = 1000)).to(dtype = torch.float32)\\ndata_dim = means.shape[1]\\nprint(\\'data_dim\\',data_dim)\\n\\nblah = pd.DataFrame(means)\\npdsm(blah)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dataset\n",
    "dataset = args.data\n",
    "dataset = 'cifar10'\n",
    "means  = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = train_kernel_size)).to(dtype = torch.float32)\n",
    "data_dim = means.shape[1]\n",
    "# dataset = 'swissroll_6D_xy1'\n",
    "\"\"\"\"\" # not used anymore since our data is pictures\n",
    "means  = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = 1000)).to(dtype = torch.float32)\n",
    "data_dim = means.shape[1]\n",
    "print('data_dim',data_dim)\n",
    "\n",
    "blah = pd.DataFrame(means)\n",
    "pdsm(blah)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_447013/367874541.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_samples = torch.tensor(p_samples).to(dtype = torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.) tensor(1.)\n",
      "ERROR: Could not find file /var/tmp/ipykernel_447013/2379803456.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_447013/367874541.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  centers  = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = train_kernel_size)).to(dtype = torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-1.) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-1.) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-1.) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-1.) tensor(0.9843)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-0.9451) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7ff7466171c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 228, in _releaseLock\n",
      "    def _releaseLock():\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/memory_profiler.py\", line 791, in trace_memory_usage\n",
      "    def trace_memory_usage(self, frame, event, arg):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Training the score network\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactornet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_samples_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m formatted_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Format the average with up to 1e-3 precision\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBefore train, Average total_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformatted_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:1188\u001b[0m, in \u001b[0;36mprofile.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(wrapped\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1187\u001b[0m     prof \u001b[38;5;241m=\u001b[39m get_prof()\n\u001b[0;32m-> 1188\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mprof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     show_results_bound(prof)\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:761\u001b[0m, in \u001b[0;36mLineProfiler.wrap_function.<locals>.f\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_ctxmgr():\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(factornet, kernel_centers, num_test_sample)\u001b[0m\n\u001b[1;32m      9\u001b[0m p_samples \u001b[38;5;241m=\u001b[39m toy_data\u001b[38;5;241m.\u001b[39minf_train_gen(dataset,batch_size \u001b[38;5;241m=\u001b[39m num_test_sample)\n\u001b[1;32m     10\u001b[0m testing_samples \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(p_samples)\u001b[38;5;241m.\u001b[39mto(dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[43mLearnCholesky\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_implicit_matching\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactornet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtesting_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkernel_centers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m total_loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     13\u001b[0m  \u001b[38;5;66;03m# Free up memory\u001b[39;00m\n",
      "File \u001b[0;32m~/wpo_distill/function_cpu.py:263\u001b[0m, in \u001b[0;36mscore_implicit_matching\u001b[0;34m(factornet, samples, centers)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcenters requires grad: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcenters\u001b[38;5;241m.\u001b[39mrequires_grad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m#laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m laplacian_over_density \u001b[38;5;241m=\u001b[39m \u001b[43mlaplacian_mog_density_div_density_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprecisions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m gradient_eval_log \u001b[38;5;241m=\u001b[39m grad_log_mog_density(samples,centers,precisions)\n\u001b[1;32m    265\u001b[0m gradient_eval_log_squared \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(gradient_eval_log \u001b[38;5;241m*\u001b[39m gradient_eval_log, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/wpo_distill/function_cpu.py:241\u001b[0m, in \u001b[0;36mlaplacian_mog_density_div_density_chunked\u001b[0;34m(x, means, precisions, chunk_size)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), chunk_size):\n\u001b[1;32m    240\u001b[0m     x_chunk \u001b[38;5;241m=\u001b[39m x[i:i\u001b[38;5;241m+\u001b[39mchunk_size]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mrequires_grad_(x\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[0;32m--> 241\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlaplacian_mog_density_div_density\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecisions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;28;01melse\u001b[39;00m result)\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m result, x_chunk\n",
      "File \u001b[0;32m~/wpo_distill/function_cpu.py:194\u001b[0m, in \u001b[0;36mlaplacian_mog_density_div_density\u001b[0;34m(x, means, precisions)\u001b[0m\n\u001b[1;32m    191\u001b[0m batch_size, num_components \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), means\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Create a batch of Multivariate Normal distributions for each component\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m mvns \u001b[38;5;241m=\u001b[39m \u001b[43mMultivariateNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecisions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each component\u001b[39;00m\n\u001b[1;32m    197\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m mvns\u001b[38;5;241m.\u001b[39mlog_prob(x)  \u001b[38;5;66;03m# Shape: (batch_size, num_components)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/distributions/multivariate_normal.py:189\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbroadcasted_scale_tril \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky(covariance_matrix)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# precision_matrix is not None\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbroadcasted_scale_tril \u001b[38;5;241m=\u001b[39m \u001b[43m_precision_to_scale_tril\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecision_matrix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/distributions/multivariate_normal.py:81\u001b[0m, in \u001b[0;36m_precision_to_scale_tril\u001b[0;34m(P)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_precision_to_scale_tril\u001b[39m(P):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Ref: https://nbviewer.jupyter.org/gist/fehiepsi/5ef8e09e61604f10607380467eb82006#Precision-to-scale_tril\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     Lf \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     L_inv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(torch\u001b[38;5;241m.\u001b[39mflip(Lf, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     83\u001b[0m     Id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(P\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mP\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mP\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "depth = args.depth\n",
    "hidden_units = args.hiddenunits\n",
    "factornet = construct_factor_model(data_dim, depth, hidden_units).to(device).to(dtype = torch.float32)\n",
    "\n",
    "lr = args.lr\n",
    "optimizer = optim.Adam(factornet.parameters(), lr=args.lr)\n",
    "\n",
    "p_samples = toy_data.inf_train_gen(dataset,batch_size = train_samples_size)\n",
    "training_samples = torch.tensor(p_samples).to(dtype = torch.float32).to(device)\n",
    "centers  = torch.tensor(toy_data.inf_train_gen(dataset, batch_size = train_kernel_size)).to(dtype = torch.float32).to(device)\n",
    "\n",
    "torch.save(centers, save_directory + str(epochs) + 'epochs' + str(train_kernel_size) + 'centers.pt')\n",
    "\n",
    "epochs = args.niters\n",
    "batch_size = args.batch_size\n",
    "\n",
    "# Training the score network\n",
    "loss = evaluate_model(factornet, centers, test_samples_size)\n",
    "formatted_loss = f'{loss:.3e}'  # Format the average with up to 1e-3 precision\n",
    "print(f'Before train, Average total_loss: {formatted_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Step: 0, Loss value: 1.187e+05\n",
      "ERROR: Could not find file /var/tmp/ipykernel_447013/2379803456.py\n",
      "tensor(-0.9843) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_447013/2379803456.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  testing_samples = torch.tensor(p_samples).to(dtype = torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-0.9294) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-1.) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-1.) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-0.9922) tensor(0.9922)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-1.) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-1.) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-0.9922) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-1.) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-0.9843) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 1/50 [01:09<56:38, 69.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 2/50 [01:21<28:40, 35.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 3/50 [01:34<19:40, 25.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 4/50 [01:47<15:36, 20.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 5/50 [01:59<13:06, 17.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 6/50 [02:11<11:32, 15.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 7/50 [04:55<46:02, 64.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 8/50 [05:08<33:24, 47.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 9/50 [05:20<25:03, 36.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 10/50 [05:33<19:26, 29.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 11/50 [05:45<15:42, 24.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 12/50 [05:58<13:03, 20.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 13/50 [06:10<11:10, 18.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 14/50 [06:23<09:50, 16.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 15/50 [06:35<08:51, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 16/50 [06:48<08:13, 14.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 17/50 [07:00<07:36, 13.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 18/50 [07:13<07:09, 13.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 19/50 [07:25<06:47, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 19 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 20/50 [07:38<06:28, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 21/50 [07:50<06:11, 12.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 21 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 22/50 [08:03<05:54, 12.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 22 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 23/50 [08:15<05:39, 12.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 23 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 24/50 [08:27<05:24, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 24 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 25/50 [08:40<05:15, 12.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 25 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 26/50 [08:52<05:00, 12.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 26 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 27/50 [09:05<04:47, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 27 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 28/50 [09:17<04:33, 12.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 28 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 29/50 [09:29<04:20, 12.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 29 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 30/50 [09:42<04:09, 12.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 31/50 [09:54<03:56, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 31 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 32/50 [10:07<03:43, 12.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 32 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 33/50 [10:19<03:32, 12.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 33 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 34/50 [10:32<03:18, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 34 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 35/50 [10:45<03:08, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 35 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 36/50 [10:57<02:54, 12.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 36 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 37/50 [11:09<02:41, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 37 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|███████▌  | 38/50 [11:22<02:29, 12.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 38 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|███████▊  | 39/50 [11:34<02:16, 12.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 39 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 40/50 [11:47<02:05, 12.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 41/50 [11:59<01:52, 12.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 41 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 42/50 [12:12<01:39, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 42 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|████████▌ | 43/50 [12:24<01:27, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 43 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 44/50 [12:37<01:14, 12.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 44 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 45/50 [12:49<01:02, 12.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 45 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 46/50 [13:02<00:49, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 46 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|█████████▍| 47/50 [13:14<00:37, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 47 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▌| 48/50 [13:27<00:24, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 48 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 49/50 [13:39<00:12, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 49 started\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [13:51<00:00, 16.64s/it]\n"
     ]
    }
   ],
   "source": [
    "#@profile\n",
    "def opt_check(factornet, samples, centers):\n",
    "    loss = LearnCholesky.score_implicit_matching(factornet,samples,centers)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Use trange instead of range to get tqdm progress bar\n",
    "for step in trange(epochs, desc=\"Training\"):\n",
    "    print(f\"Step {step} started\")\n",
    "    randind = torch.randint(0, train_samples_size, [batch_size,])\n",
    "    samples = training_samples[randind, :]\n",
    "    loss_value = opt_check(factornet, samples, centers)\n",
    "\n",
    "    if step % 4000 == 0:\n",
    "        print(f'Step: {step}, Loss value: {loss_value:.3e}')\n",
    "    \n",
    "    if step % 20000 == 0:\n",
    "        loss0 = evaluate_model(factornet, centers, test_samples_size)\n",
    "        save_training_slice_cov(factornet, centers, step, lr, batch_size, loss0, save_directory)\n",
    "        \n",
    "    if step < epochs - 1:\n",
    "        del samples\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file /var/tmp/ipykernel_447013/2379803456.py\n",
      "tensor(-0.9686) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_447013/2379803456.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  testing_samples = torch.tensor(p_samples).to(dtype = torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-1.) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-1.) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-1.) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-1.) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-1.) tensor(0.9922)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-0.9608) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-1.) tensor(0.9608)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-0.9686) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "tensor(-0.9529) tensor(1.)\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "After train, Average total_loss: -9.924e+03\n"
     ]
    }
   ],
   "source": [
    "loss = evaluate_model(factornet, centers, test_samples_size)    \n",
    "save_training_slice_cov(factornet, centers, step, lr, batch_size, loss0, save_directory)\n",
    "formatted_loss = f'{loss:.3e}'  # Format the average with up to 1e-3 precision\n",
    "print(f'After train, Average total_loss: {formatted_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file /var/tmp/ipykernel_174720/1267574142.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   3825.4 MiB   3825.4 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   3825.4 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30   7425.4 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31   7497.4 MiB     72.1 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32   7497.4 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  11152.4 MiB   3655.0 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  11152.4 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/wpo_distill/function_cpu.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast('cpu', dtype=torch.bfloat16):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  11152.4 MiB  11152.4 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  11152.4 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  11152.4 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  21955.3 MiB  10802.9 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  21969.8 MiB     14.5 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  21969.8 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  21969.8 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  21969.8 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  21970.6 MiB      0.8 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  21975.2 MiB      4.5 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  21975.2 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  21975.2 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  21975.2 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  21975.2 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  21975.2 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  21975.2 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  21975.4 MiB      0.2 MiB           1       gc.collect()\n",
      "   230  21975.4 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  21975.4 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  11152.4 MiB  11152.4 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  11152.4 MiB      0.0 MiB           1       results = []\n",
      "   239  21975.4 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  11152.4 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  21975.4 MiB  10823.0 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  21975.4 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  21975.4 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  21975.4 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  21975.4 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  21975.4 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  21975.4 MiB  21975.4 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  21975.4 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  21975.4 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  21975.4 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  32787.4 MiB  10812.0 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  32796.7 MiB      9.4 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  32796.7 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  32796.7 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  32796.7 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  32796.7 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  32796.7 MiB      0.0 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  32801.0 MiB      4.3 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  32801.0 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  32801.0 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  32801.0 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  32801.0 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   2002.3 MiB   2002.3 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   2002.3 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   2002.3 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   3825.4 MiB   1823.1 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  11152.4 MiB   7327.0 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  32801.0 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  11152.4 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  11152.4 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  11152.4 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  21975.4 MiB  10823.0 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  32801.0 MiB  10825.7 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  32801.0 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  32801.0 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  32801.0 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  32801.0 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  32801.0 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  32801.0 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "Step: 0, Loss value: 4.109e+04\n",
      "ERROR: Could not find file /var/tmp/ipykernel_174720/4110140829.py\n",
      "tensor(-1.) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_174720/4110140829.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  testing_samples = torch.tensor(p_samples).to(dtype = torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   7563.0 MiB   7563.0 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   7563.0 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11163.0 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11235.0 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11235.0 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  14834.9 MiB   3599.9 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  14834.9 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  14834.9 MiB  14834.9 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  14834.9 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  14834.9 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  25634.9 MiB  10800.0 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  25640.5 MiB      5.6 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  25640.5 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  25640.5 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  25646.4 MiB      5.9 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  25652.3 MiB      5.9 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  25658.3 MiB      6.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  25658.3 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  25658.3 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  25658.3 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  25658.3 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  25658.3 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  25658.3 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  25658.5 MiB      0.2 MiB           1       gc.collect()\n",
      "   230  25658.5 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  25658.5 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  14834.9 MiB  14834.9 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  14834.9 MiB      0.0 MiB           1       results = []\n",
      "   239  25658.5 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  14834.9 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  25658.5 MiB  10823.7 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  25658.5 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  25658.5 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  25658.5 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  25658.5 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  25658.5 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  25658.5 MiB  25658.5 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  25658.5 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  25658.5 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  25658.5 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36477.9 MiB  10819.3 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36495.3 MiB     17.4 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36495.3 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36495.3 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36501.3 MiB      6.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36501.3 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36507.4 MiB      6.1 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36513.2 MiB      5.8 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36513.2 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36513.2 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36513.2 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36513.2 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   5762.3 MiB   5762.3 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   5762.3 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   5762.3 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   7563.0 MiB   1800.7 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  14834.9 MiB   7271.9 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36513.2 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  14834.9 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  14834.9 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  14834.9 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  25658.5 MiB  10823.7 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36513.2 MiB  10854.7 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36513.2 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36513.2 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36513.2 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36513.2 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36513.2 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36513.2 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-1.) tensor(1.)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   7649.0 MiB   7649.0 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   7649.0 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11249.0 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11321.0 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11321.0 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  14960.6 MiB   3639.5 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  14960.6 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  14960.6 MiB  14960.6 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  14960.6 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  14960.6 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  25760.6 MiB  10800.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  25760.2 MiB     -0.5 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  25760.2 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  25760.2 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  25760.2 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  25760.3 MiB      0.1 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  25760.3 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  25760.3 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  25760.3 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  25760.3 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  25760.3 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  25760.3 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  25760.3 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  25760.3 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  25760.3 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  25760.3 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  14960.6 MiB  14960.6 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  14960.6 MiB      0.0 MiB           1       results = []\n",
      "   239  25760.3 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  14960.6 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  25760.3 MiB  10799.8 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  25760.3 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  25760.3 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  25760.3 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  25760.3 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  25760.3 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  25760.3 MiB  25760.3 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  25760.3 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  25760.3 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  25760.3 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36579.1 MiB  10818.8 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36579.9 MiB      0.8 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36579.9 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36579.9 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36579.9 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36579.9 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36580.1 MiB      0.1 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36580.1 MiB      0.1 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36580.1 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36580.1 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36580.1 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36580.1 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   5835.0 MiB   5835.0 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   5835.0 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   5835.0 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   7649.0 MiB   1814.0 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  14960.6 MiB   7311.6 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36580.1 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  14960.6 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  14960.6 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  14960.6 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  25760.3 MiB  10799.8 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36580.1 MiB  10819.8 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36580.1 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36580.1 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36580.1 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36580.1 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36580.1 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36580.1 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-1.) tensor(1.)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   7708.2 MiB   7708.2 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   7708.2 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11308.2 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11380.2 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11380.2 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15017.3 MiB   3637.1 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15017.3 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15017.3 MiB  15017.3 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15017.3 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15017.3 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  25817.4 MiB  10800.0 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  25817.3 MiB     -0.1 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  25817.3 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  25817.3 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  25817.3 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  25817.4 MiB      0.1 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  25817.4 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  25817.4 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  25817.4 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  25817.4 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  25817.4 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  25817.4 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  25817.4 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  25817.4 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  25817.4 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  25817.4 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15017.3 MiB  15017.3 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15017.3 MiB      0.0 MiB           1       results = []\n",
      "   239  25817.4 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15017.3 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  25817.4 MiB  10800.1 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  25817.4 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  25817.4 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  25817.4 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  25817.4 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  25817.4 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  25817.4 MiB  25817.4 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  25817.4 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  25817.4 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  25817.4 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36636.3 MiB  10818.9 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36637.0 MiB      0.7 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36637.0 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36637.0 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36637.0 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36637.0 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36637.0 MiB      0.0 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36637.1 MiB      0.1 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36637.1 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36637.1 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36637.1 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36637.1 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   5896.9 MiB   5896.9 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   5896.9 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   5896.9 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   7708.2 MiB   1811.3 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15017.3 MiB   7309.1 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36637.1 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15017.3 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15017.3 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15017.3 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  25817.4 MiB  10800.1 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36637.1 MiB  10819.7 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36637.1 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36637.1 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36637.1 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36637.1 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36637.1 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36637.1 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-1.) tensor(1.)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   7758.9 MiB   7758.9 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   7758.9 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11358.9 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11430.9 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11430.9 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15067.7 MiB   3636.8 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15067.7 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15067.7 MiB  15067.7 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15067.7 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15067.7 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  25869.4 MiB  10801.7 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  25869.2 MiB     -0.2 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  25869.2 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  25869.2 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  25869.2 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  25869.3 MiB      0.1 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  25869.3 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  25869.3 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  25869.3 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  25869.3 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  25869.3 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  25869.3 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  25869.3 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  25869.3 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  25869.3 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  25869.3 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15067.7 MiB  15067.7 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15067.7 MiB      0.0 MiB           1       results = []\n",
      "   239  25869.3 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15067.7 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  25869.3 MiB  10801.6 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  25869.3 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  25869.3 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  25869.3 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  25869.3 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  25869.3 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  25869.3 MiB  25869.3 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  25869.3 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  25869.3 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  25869.3 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36676.4 MiB  10807.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36681.6 MiB      5.2 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36681.6 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36681.6 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36681.6 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36681.6 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36687.9 MiB      6.3 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36693.4 MiB      5.5 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36693.4 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36693.4 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36693.4 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36693.4 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   5958.0 MiB   5958.0 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   5958.0 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   5958.0 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   7758.9 MiB   1800.9 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15067.7 MiB   7308.8 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36693.4 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15067.7 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15067.7 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15067.7 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  25869.3 MiB  10801.6 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36693.4 MiB  10824.1 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36693.4 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36693.4 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36693.4 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36693.4 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36693.4 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36693.4 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-0.9451) tensor(0.9843)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   7800.1 MiB   7800.1 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   7800.1 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11400.1 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11472.1 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11472.1 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15111.7 MiB   3639.6 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15111.7 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15111.7 MiB  15111.7 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15111.7 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15111.7 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  25911.7 MiB  10800.0 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  25911.7 MiB     -0.0 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  25911.7 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  25911.7 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  25911.7 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  25911.8 MiB      0.1 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  25917.1 MiB      5.3 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  25917.1 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  25917.1 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  25917.1 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  25917.1 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  25917.1 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  25917.1 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  25917.3 MiB      0.2 MiB           1       gc.collect()\n",
      "   230  25917.3 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  25917.3 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15111.7 MiB  15111.7 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15111.7 MiB      0.0 MiB           1       results = []\n",
      "   239  25917.3 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15111.7 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  25917.3 MiB  10805.7 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  25917.3 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  25917.3 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  25917.3 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  25917.3 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  25917.3 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  25917.3 MiB  25917.3 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  25917.3 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  25917.3 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  25917.3 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36739.4 MiB  10822.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36756.8 MiB     17.4 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36756.8 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36756.8 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36762.9 MiB      6.1 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36762.9 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36768.9 MiB      6.1 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36774.8 MiB      5.8 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36774.8 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36774.8 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36774.8 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36774.8 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   5985.9 MiB   5985.9 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   5985.9 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   5985.9 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   7800.1 MiB   1814.2 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15111.7 MiB   7311.6 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36774.8 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15111.7 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15111.7 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15111.7 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  25917.3 MiB  10805.7 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36774.8 MiB  10857.4 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36774.8 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36774.8 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36774.8 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36774.8 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36774.8 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36774.8 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-1.) tensor(1.)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   7910.5 MiB   7910.5 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   7910.5 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11510.5 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11582.5 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11582.5 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15223.5 MiB   3640.9 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15223.5 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15223.5 MiB  15223.5 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15223.5 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15223.5 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  26025.3 MiB  10801.8 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  26024.9 MiB     -0.4 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  26024.9 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  26024.9 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  26024.9 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  26025.1 MiB      0.1 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  26025.1 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  26025.1 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  26025.1 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  26025.1 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  26025.1 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  26025.1 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  26025.1 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  26025.1 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  26025.1 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  26025.1 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15223.5 MiB  15223.5 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15223.5 MiB      0.0 MiB           1       results = []\n",
      "   239  26025.1 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15223.5 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  26025.1 MiB  10801.6 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  26025.1 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  26025.1 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  26025.1 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  26025.1 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  26025.1 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  26025.1 MiB  26025.1 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  26025.1 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  26025.1 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  26025.1 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36835.1 MiB  10810.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36835.2 MiB      0.0 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36835.2 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36835.2 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36835.2 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36835.2 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36835.4 MiB      0.2 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36835.4 MiB      0.0 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36835.4 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36835.4 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36835.4 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36835.4 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   6096.5 MiB   6096.5 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   6096.5 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   6096.5 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   7910.5 MiB   1814.0 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15223.5 MiB   7313.0 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36835.4 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15223.5 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15223.5 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15223.5 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  26025.1 MiB  10801.6 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36835.4 MiB  10810.3 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36835.4 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36835.4 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36835.4 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36835.4 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36835.4 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36835.4 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-1.) tensor(0.9922)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   7955.4 MiB   7955.4 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   7955.4 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11555.4 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11627.5 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11627.5 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15260.3 MiB   3632.8 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15260.3 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15260.3 MiB  15260.3 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15260.3 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15260.3 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  26062.4 MiB  10802.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  26062.1 MiB     -0.2 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  26062.1 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  26062.1 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  26062.1 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  26062.5 MiB      0.3 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  26062.5 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  26062.5 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  26062.5 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  26062.5 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  26062.5 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  26062.5 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  26062.5 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  26062.5 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  26062.5 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  26062.5 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15260.3 MiB  15260.3 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15260.3 MiB      0.0 MiB           1       results = []\n",
      "   239  26062.5 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15260.3 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  26062.5 MiB  10802.2 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  26062.5 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  26062.5 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  26062.5 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  26062.5 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  26062.5 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  26062.5 MiB  26062.5 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  26062.5 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  26062.5 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  26062.5 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36867.4 MiB  10804.9 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36867.6 MiB      0.2 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36867.6 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36867.6 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36867.6 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36867.6 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36867.7 MiB      0.2 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36873.1 MiB      5.4 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36873.1 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36873.1 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36873.1 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36873.1 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   6152.0 MiB   6152.0 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   6152.0 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   6152.0 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   7955.4 MiB   1803.4 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15260.3 MiB   7304.9 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36873.1 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15260.3 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15260.3 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15260.3 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  26062.5 MiB  10802.2 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36873.1 MiB  10810.6 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36873.1 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36873.1 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36873.1 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36873.1 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36873.1 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36873.1 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-1.) tensor(1.)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   8003.9 MiB   8003.9 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   8003.9 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11604.0 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11676.0 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11677.9 MiB      1.9 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15296.7 MiB   3618.8 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15296.7 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15296.7 MiB  15296.7 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15296.7 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15296.7 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  26098.8 MiB  10802.1 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  26098.5 MiB     -0.3 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  26098.5 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  26098.5 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  26098.5 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  26098.6 MiB      0.1 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  26098.6 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  26098.6 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  26098.6 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  26098.6 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  26098.6 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  26098.6 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  26098.6 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  26098.6 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  26098.6 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  26098.6 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15296.7 MiB  15296.7 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15296.7 MiB      0.0 MiB           1       results = []\n",
      "   239  26098.6 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15296.7 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  26098.6 MiB  10802.0 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  26098.6 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  26098.6 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  26098.6 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  26098.6 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  26098.6 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  26098.6 MiB  26098.6 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  26098.6 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  26098.6 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  26098.6 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36914.1 MiB  10815.5 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36914.3 MiB      0.2 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36914.3 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36914.3 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36914.3 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36914.3 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36914.3 MiB      0.0 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36914.3 MiB      0.0 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36914.3 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36914.3 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36914.3 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36914.3 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   6200.8 MiB   6200.8 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   6200.8 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   6200.8 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   8003.9 MiB   1803.1 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15296.7 MiB   7292.7 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36914.3 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15296.7 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15296.7 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15296.7 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  26098.6 MiB  10802.0 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36914.3 MiB  10815.7 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36914.3 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36914.3 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36914.3 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36914.3 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36914.3 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36914.3 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-0.9843) tensor(0.8510)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   8048.4 MiB   8048.4 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   8048.4 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11650.3 MiB   3601.9 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11722.3 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11722.3 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15362.4 MiB   3640.1 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15362.4 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   188  15362.4 MiB  15362.4 MiB           1   @profile\n",
      "   189                                         def laplacian_mog_density_div_density(x, means, precisions):\n",
      "   190  15362.4 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   191  15362.4 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   192                                         \n",
      "   193                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   194  26164.4 MiB  10802.0 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   195                                         \n",
      "   196                                             # Calculate the log probabilities for each component\n",
      "   197  26164.4 MiB      0.1 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   198                                         \n",
      "   199                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   200  26164.4 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   201                                         \n",
      "   202                                             # Calculate the softmax probabilities along the components dimension\n",
      "   203  26164.4 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   204                                         \n",
      "   205  26164.5 MiB      0.1 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   206                                         \n",
      "   207                                             # Calculate the covariance matrix term\n",
      "   208                                             #cov_term = torch.matmul(precisions, x_mean.unsqueeze(-1)).squeeze(-1)  # Shape: (batch_size, num_components, 2)\n",
      "   209  26164.7 MiB      0.1 MiB           1       cov_term = torch.einsum(\"kij,bkj->bki\", precisions, x_mean)\n",
      "   210                                             \n",
      "   211                                             # Calculate the squared linear term\n",
      "   212  26164.7 MiB      0.0 MiB           1       squared_linear = torch.sum(cov_term * cov_term, dim=-1)  # Shape: (batch_size, num_components)\n",
      "   213                                         \n",
      "   214                                             # Calculate the trace of the precision matrix term\n",
      "   215  26164.7 MiB      0.0 MiB           1       trace_precision = torch.diagonal(precisions, dim1=-2, dim2=-1)  # Shape: (num_components, 2)\n",
      "   216  26164.7 MiB      0.0 MiB           1       trace_precision = trace_precision.sum(dim=-1)  # Shape: (num_components,)\n",
      "   217                                         \n",
      "   218                                             # Expand dimensions for broadcasting\n",
      "   219  26164.7 MiB      0.0 MiB           1       trace_precision = trace_precision.unsqueeze(0)  # Shape: (1, num_components)\n",
      "   220                                         \n",
      "   221                                             # Calculate the Laplacian component\n",
      "   222  26164.7 MiB      0.0 MiB           1       laplacian_component = softmax_probs * (squared_linear - trace_precision)\n",
      "   223                                         \n",
      "   224                                             # Sum over components to obtain the Laplacian of the density over the density\n",
      "   225  26164.7 MiB      0.0 MiB           1       laplacian_over_density = torch.sum(laplacian_component, dim=1)  # Shape: (batch_size,)\n",
      "   226                                             \n",
      "   227                                             #free up memory \n",
      "   228  26164.7 MiB      0.0 MiB           1       del mvns, log_probs, log_sum_exp, softmax_probs, x_mean, cov_term\n",
      "   229  26164.7 MiB      0.0 MiB           1       gc.collect()\n",
      "   230  26164.7 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   231  26164.7 MiB      0.0 MiB           1       return laplacian_over_density\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   236  15362.4 MiB  15362.4 MiB           1   @profile\n",
      "   237                                         def laplacian_mog_density_div_density_chunked(x, means, precisions, chunk_size=16):\n",
      "   238  15362.4 MiB      0.0 MiB           1       results = []\n",
      "   239  26164.7 MiB      0.0 MiB           2       for i in range(0, x.size(0), chunk_size):\n",
      "   240  15362.4 MiB      0.0 MiB           1           x_chunk = x[i:i+chunk_size].detach().clone().requires_grad_(x.requires_grad)\n",
      "   241  26164.7 MiB  10802.3 MiB           1           result = laplacian_mog_density_div_density(x_chunk, means, precisions)\n",
      "   242  26164.7 MiB      0.0 MiB           1           results.append(result.detach() if not result.requires_grad else result)\n",
      "   243  26164.7 MiB      0.0 MiB           1           del result, x_chunk\n",
      "   244  26164.7 MiB      0.0 MiB           1           gc.collect()\n",
      "   245  26164.7 MiB      0.0 MiB           1           torch.cuda.empty_cache()\n",
      "   246  26164.7 MiB      0.0 MiB           1       return torch.cat(results, dim=0)\n",
      "\n",
      "\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   106  26164.7 MiB  26164.7 MiB           1   @profile\n",
      "   107                                         def grad_log_mog_density(x, means, precisions):\n",
      "   108  26164.7 MiB      0.0 MiB           1       x = x.unsqueeze(1)  # Shape: (batch_size, 1, 2)\n",
      "   109  26164.7 MiB      0.0 MiB           1       dim = x.size(-1)\n",
      "   110  26164.7 MiB      0.0 MiB           1       batch_size, num_components = x.size(0), means.size(0)\n",
      "   111                                         \n",
      "   112                                             # Create a batch of Multivariate Normal distributions for each component\n",
      "   113  36985.0 MiB  10820.3 MiB           1       mvns = MultivariateNormal(loc=means, precision_matrix=precisions)\n",
      "   114                                         \n",
      "   115                                             # Calculate the log probabilities for each component\n",
      "   116  36986.0 MiB      1.0 MiB           1       log_probs = mvns.log_prob(x)  # Shape: (batch_size, num_components)\n",
      "   117                                         \n",
      "   118                                             # Use torch.logsumexp to compute the log of the sum of exponentiated log probabilities\n",
      "   119  36986.0 MiB      0.0 MiB           1       log_sum_exp = torch.logsumexp(log_probs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
      "   120                                         \n",
      "   121                                             # Calculate the softmax probabilities along the components dimension\n",
      "   122                                             \n",
      "   123  36986.0 MiB      0.0 MiB           1       softmax_probs = torch.softmax(log_probs, dim=1)  # Shape: (batch_size, num_components)\n",
      "   124  36986.0 MiB      0.0 MiB           1       x_mean = x - means  # Shape: (batch_size, num_components, 2)\n",
      "   125  36986.0 MiB      0.0 MiB           1       \"\"\"\n",
      "   126                                             x_mean_reshaped = x_mean.view(batch_size, num_components, dim, 1)\n",
      "   127                                             precision_matrix = precisions.unsqueeze(0)  # Shape: (1, num_components, 2, 2)\n",
      "   128                                             precision_matrix = precision_matrix.expand(x.shape[0], -1, -1, -1)  # Shape: (batch_size, num_components, 2, 2)\n",
      "   129                                         \n",
      "   130                                             x_mean_cov = torch.matmul(precision_matrix, x_mean_reshaped).squeeze(dim = -1)\n",
      "   131                                             print(f\"matmul mean cov: {x_mean_cov}\")\n",
      "   132                                             del x_mean_cov\n",
      "   133                                             gc.collect()\n",
      "   134                                             torch.cuda.empty_cache()\n",
      "   135                                             \"\"\"\n",
      "   136                                             # calculate without resizing (more memory efficient)\n",
      "   137  36986.2 MiB      0.2 MiB           1       x_mean_cov = torch.einsum('kij,bkj->bki', precisions, x_mean)\n",
      "   138                                             \n",
      "   139                                             # Calculate the gradient of log density with respect to x\n",
      "   140                                         \n",
      "   141  36991.3 MiB      5.1 MiB           1       gradient = -torch.sum(softmax_probs.unsqueeze(-1) * x_mean_cov, dim=1)\n",
      "   142  36991.3 MiB      0.0 MiB           1       del x_mean_cov\n",
      "   143  36991.3 MiB      0.0 MiB           1       gc.collect()\n",
      "   144  36991.3 MiB      0.0 MiB           1       torch.cuda.empty_cache()\n",
      "   145  36991.3 MiB      0.0 MiB           1       return gradient\n",
      "\n",
      "\n",
      "laplacian_over_density requires grad: True\n",
      "gradient_eval_log requires grad: True\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   248   6241.1 MiB   6241.1 MiB           1   @profile\n",
      "   249                                         def score_implicit_matching(factornet,samples,centers):\n",
      "   250                                             # detach the samples and centers\n",
      "   251                                             #samples = samples.detach()\n",
      "   252                                             #centers = centers.detach()\n",
      "   253   6241.1 MiB      0.0 MiB           1       centers.requires_grad_(True)\n",
      "   254   6241.1 MiB      0.0 MiB           1       dim = centers.shape[-1]\n",
      "   255   8048.4 MiB   1807.3 MiB           1       factor_eval = checkpoint(factornet,centers)\n",
      "   256  15362.4 MiB   7314.0 MiB           1       precisions = vectors_to_precision(factor_eval,dim)\n",
      "   257                                             #with autocast(\"cuda\"):\n",
      "   258  36991.3 MiB      0.0 MiB           2       with autocast('cpu', dtype=torch.bfloat16):\n",
      "   259  15362.4 MiB      0.0 MiB           1           print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   260  15362.4 MiB      0.0 MiB           1           print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   261  15362.4 MiB      0.0 MiB           1           print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   262                                                 #laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   263  26164.7 MiB  10802.3 MiB           1           laplacian_over_density = laplacian_mog_density_div_density_chunked(samples,centers,precisions)\n",
      "   264  36991.3 MiB  10826.7 MiB           1           gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   265  36991.3 MiB      0.0 MiB           1           gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   266  36991.3 MiB      0.0 MiB           1           print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   267  36991.3 MiB      0.0 MiB           1           print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   268  36991.3 MiB      0.0 MiB           1       \"\"\"\n",
      "   269                                             # evaluate factor net\n",
      "   270                                             factor_eval = factornet(centers) \n",
      "   271                                             # create precision matrix from the cholesky factor\n",
      "   272                                             dim = centers.shape[-1]\n",
      "   273                                             precisions = vectors_to_precision(factor_eval,dim)\n",
      "   274                                             print(f\"precisions requires grad: {precisions.requires_grad}\")\n",
      "   275                                             print(f\"samples requires grad: {samples.requires_grad}\")\n",
      "   276                                             print(f\"centers requires grad: {centers.requires_grad}\")\n",
      "   277                                             laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\n",
      "   278                                             gradient_eval_log = grad_log_mog_density(samples,centers,precisions)\n",
      "   279                                             # square gradient\n",
      "   280                                             \n",
      "   281                                             gradient_eval_log_squared = torch.sum(gradient_eval_log * gradient_eval_log, dim=1)\n",
      "   282                                             print(f\"laplacian_over_density requires grad: {laplacian_over_density.requires_grad}\")\n",
      "   283                                             print(f\"gradient_eval_log requires grad: {gradient_eval_log.requires_grad}\")\n",
      "   284                                             \"\"\"\n",
      "   285                                             #loss function\n",
      "   286  36991.3 MiB      0.0 MiB           1       loss = (2 * laplacian_over_density - gradient_eval_log_squared)\n",
      "   287                                         \n",
      "   288  36991.3 MiB      0.0 MiB           1       return loss.mean(dim =0)\n",
      "\n",
      "\n",
      "tensor(-1.) tensor(1.)\n",
      "Filename: /home/jupyter/wpo_distill/function_cpu.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    16   8093.8 MiB   8093.8 MiB           1   @profile\n",
      "    17                                         def vectors_to_precision(vectors,dim):\n",
      "    18                                             \"\"\"\n",
      "    19                                             Maps an array of 1xdim vectors into Cholesky factors and returns an array of precision matrices.\n",
      "    20                                             \n",
      "    21                                             Args:\n",
      "    22                                             vectors (torch.Tensor): A tensor of shape (batch_size, 1, dim), where each 1xdim tensor represents the\n",
      "    23                                                                     lower triangular part of the Cholesky factor.\n",
      "    24                                             \n",
      "    25                                             Returns:\n",
      "    26                                             torch.Tensor: A tensor of shape (batch_size, dim, dim), containing the corresponding precision matrices.\n",
      "    27                                             \"\"\"\n",
      "    28   8093.8 MiB      0.0 MiB           1       batch_size = vectors.shape[0]\n",
      "    29                                             # Reshape the input vectors into lower triangular matrices\n",
      "    30  11693.8 MiB   3600.0 MiB           1       L = torch.zeros(batch_size, dim, dim, dtype=vectors.dtype, device=vectors.device)\n",
      "    31  11765.8 MiB     72.0 MiB           1       indices = torch.tril_indices(dim, dim)\n",
      "    32  11765.8 MiB      0.0 MiB           1       L[:, indices[0], indices[1]] = vectors.squeeze(1)\n",
      "    33                                             \n",
      "    34                                             # Construct the precision matrices using Cholesky factorization\n",
      "    35  15407.0 MiB   3641.2 MiB           1       C = torch.matmul(L, L.transpose(1, 2)) + 0.01 * torch.eye(dim).to(vectors.device) # (add identity matrix to maintain positive definiteness)\n",
      "    36                                             \n",
      "    37  15407.0 MiB      0.0 MiB           1       return C\n",
      "\n",
      "\n",
      "precisions requires grad: True\n",
      "samples requires grad: False\n",
      "centers requires grad: True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20000\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     loss0 \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactornet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_samples_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     save_training_slice_cov(factornet, centers, step, lr, batch_size, loss0, save_directory)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m<\u001b[39m epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     31\u001b[0m      \u001b[38;5;66;03m# Free up memory\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:1188\u001b[0m, in \u001b[0;36mprofile.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(wrapped\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1187\u001b[0m     prof \u001b[38;5;241m=\u001b[39m get_prof()\n\u001b[0;32m-> 1188\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mprof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     show_results_bound(prof)\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:761\u001b[0m, in \u001b[0;36mLineProfiler.wrap_function.<locals>.f\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_ctxmgr():\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(factornet, kernel_centers, num_test_sample)\u001b[0m\n\u001b[1;32m      9\u001b[0m p_samples \u001b[38;5;241m=\u001b[39m toy_data\u001b[38;5;241m.\u001b[39minf_train_gen(dataset,batch_size \u001b[38;5;241m=\u001b[39m num_test_sample)\n\u001b[1;32m     10\u001b[0m testing_samples \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(p_samples)\u001b[38;5;241m.\u001b[39mto(dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[43mLearnCholesky\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_implicit_matching\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactornet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtesting_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkernel_centers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m total_loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     13\u001b[0m  \u001b[38;5;66;03m# Free up memory\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:1188\u001b[0m, in \u001b[0;36mprofile.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(wrapped\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1187\u001b[0m     prof \u001b[38;5;241m=\u001b[39m get_prof()\n\u001b[0;32m-> 1188\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mprof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     show_results_bound(prof)\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:761\u001b[0m, in \u001b[0;36mLineProfiler.wrap_function.<locals>.f\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_ctxmgr():\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wpo_distill/function_cpu.py:263\u001b[0m, in \u001b[0;36mscore_implicit_matching\u001b[0;34m(factornet, samples, centers)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcenters requires grad: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcenters\u001b[38;5;241m.\u001b[39mrequires_grad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m#laplacian_over_density = laplacian_mog_density_div_density(samples,centers,precisions)\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m laplacian_over_density \u001b[38;5;241m=\u001b[39m \u001b[43mlaplacian_mog_density_div_density_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprecisions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m gradient_eval_log \u001b[38;5;241m=\u001b[39m grad_log_mog_density(samples,centers,precisions)\n\u001b[1;32m    265\u001b[0m gradient_eval_log_squared \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(gradient_eval_log \u001b[38;5;241m*\u001b[39m gradient_eval_log, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:1188\u001b[0m, in \u001b[0;36mprofile.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(wrapped\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1187\u001b[0m     prof \u001b[38;5;241m=\u001b[39m get_prof()\n\u001b[0;32m-> 1188\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mprof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     show_results_bound(prof)\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:761\u001b[0m, in \u001b[0;36mLineProfiler.wrap_function.<locals>.f\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_ctxmgr():\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wpo_distill/function_cpu.py:241\u001b[0m, in \u001b[0;36mlaplacian_mog_density_div_density_chunked\u001b[0;34m(x, means, precisions, chunk_size)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), chunk_size):\n\u001b[1;32m    240\u001b[0m     x_chunk \u001b[38;5;241m=\u001b[39m x[i:i\u001b[38;5;241m+\u001b[39mchunk_size]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mrequires_grad_(x\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[0;32m--> 241\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlaplacian_mog_density_div_density\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecisions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;28;01melse\u001b[39;00m result)\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m result, x_chunk\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:1188\u001b[0m, in \u001b[0;36mprofile.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(wrapped\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1187\u001b[0m     prof \u001b[38;5;241m=\u001b[39m get_prof()\n\u001b[0;32m-> 1188\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mprof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     show_results_bound(prof)\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/memory_profiler.py:761\u001b[0m, in \u001b[0;36mLineProfiler.wrap_function.<locals>.f\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_ctxmgr():\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wpo_distill/function_cpu.py:194\u001b[0m, in \u001b[0;36mlaplacian_mog_density_div_density\u001b[0;34m(x, means, precisions)\u001b[0m\n\u001b[1;32m    191\u001b[0m batch_size, num_components \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), means\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Create a batch of Multivariate Normal distributions for each component\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m mvns \u001b[38;5;241m=\u001b[39m \u001b[43mMultivariateNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecisions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each component\u001b[39;00m\n\u001b[1;32m    197\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m mvns\u001b[38;5;241m.\u001b[39mlog_prob(x)  \u001b[38;5;66;03m# Shape: (batch_size, num_components)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/distributions/multivariate_normal.py:189\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbroadcasted_scale_tril \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky(covariance_matrix)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# precision_matrix is not None\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbroadcasted_scale_tril \u001b[38;5;241m=\u001b[39m \u001b[43m_precision_to_scale_tril\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecision_matrix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/distributions/multivariate_normal.py:81\u001b[0m, in \u001b[0;36m_precision_to_scale_tril\u001b[0;34m(P)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_precision_to_scale_tril\u001b[39m(P):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Ref: https://nbviewer.jupyter.org/gist/fehiepsi/5ef8e09e61604f10607380467eb82006#Precision-to-scale_tril\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     Lf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     82\u001b[0m     L_inv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(torch\u001b[38;5;241m.\u001b[39mflip(Lf, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     83\u001b[0m     Id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(P\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mP\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mP\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "for step in range(epochs):\n",
    "    start = time.time()\n",
    "    # samples_toydata\n",
    "    randind = torch.randint(0,train_samples_size,[batch_size,])\n",
    "    samples = training_samples[randind,:]\n",
    "    loss_value = opt_check(factornet, samples, centers)\n",
    "    \"\"\"\n",
    "    loss = LearnCholesky.score_implicit_matching(factornet,samples,centers)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "    if not step % 4000:\n",
    "        #loss_value = loss.item()\n",
    "        print(f'Step: {step}, Loss value: {loss_value:.3e}')\n",
    "\n",
    "    if not step % 20000:\n",
    "        loss0 = evaluate_model(factornet, centers, test_samples_size)\n",
    "        save_training_slice_cov(factornet, centers, step, lr, batch_size, loss0, save_directory)\n",
    "    if step < epochs - 1:\n",
    "         # Free up memory\n",
    "        del samples # del loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Elapsed time:\", time.time() - start)\n",
    "    \n",
    "loss = evaluate_model(factornet, centers, test_samples_size)    \n",
    "save_training_slice_cov(factornet, centers, step, lr, batch_size, loss0, save_directory)\n",
    "formatted_loss = f'{loss:.3e}'  # Format the average with up to 1e-3 precision\n",
    "print(f'After train, Average total_loss: {formatted_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Current working dir: /home/jupyter/wpo_distill\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-10.691046..15.402475].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-12.706252..12.815369].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-10.633032..13.250306].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-16.568491..16.274464].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-11.301103..11.333479].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-13.891276..13.010072].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-14.050747..12.091162].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-11.99921..13.46818].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-13.153183..13.368686].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-11.396852..15.081185].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I think this will not work for general (centers needs to be same as before to plot properly)\\nrandind = torch.randint(0,1000,[1000,])\\ncenters = means[randind,:].to(device)\\nprecisions = LearnCholesky.vectors_to_precision(factornet(centers),data_dim)\\n\\nLearnCholesky.scatter_samples_from_model(centers, precisions, dim1 = 0, dim2 = 1,save_path=save_directory + 'samples.png')\\nLearnCholesky.plot_density_2d_marg(centers,factornet,dim1 = 0, dim2 = 1, save_path=save_directory + 'density.png')\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from trained model\n",
    "# and plot density\n",
    "#file /wpo_distill/cifar10_experiments/test/epoch499model_weights.pth\n",
    "#factornet.load_state_dict(torch.load('wpo_distill/cifar10_experiments/test/epoch499model_weights.pth'))\n",
    "print(os.path.exists(\"/home/jupyter/wpo_distill/cifar10_experiments/test/epoch499model_weights.pth\"))\n",
    "print(\"Current working dir:\", os.getcwd())\n",
    "model = construct_factor_model(data_dim, depth, hidden_units).to(device).to(dtype = torch.float32)\n",
    "model.load_state_dict(torch.load(\"/home/jupyter/wpo_distill/cifar10_experiments/test/epoch499model_weights.pth\"))\n",
    "\n",
    "with torch.no_grad():\n",
    "        \n",
    "    precisions = LearnCholesky.vectors_to_precision(model(centers),data_dim)\n",
    "\n",
    "    LearnCholesky.plot_images(centers, precisions, epoch = 49, plot_number=10, save_path=save_directory + 'samples')\n",
    "\n",
    "\"\"\" I think this will not work for general (centers needs to be same as before to plot properly)\n",
    "randind = torch.randint(0,1000,[1000,])\n",
    "centers = means[randind,:].to(device)\n",
    "precisions = LearnCholesky.vectors_to_precision(factornet(centers),data_dim)\n",
    "\n",
    "LearnCholesky.scatter_samples_from_model(centers, precisions, dim1 = 0, dim2 = 1,save_path=save_directory + 'samples.png')\n",
    "LearnCholesky.plot_density_2d_marg(centers,factornet,dim1 = 0, dim2 = 1, save_path=save_directory + 'density.png')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LearnCholesky.scatter_samples_from_model(centers, precisions, dim1 = 2, dim2 = 3, save_path=save_directory + 'samples.png')\n",
    "# LearnCholesky.plot_density_2d_marg(centers,factornet, dim1 = 2, dim2 = 3, save_path=save_directory + 'density.png')\n",
    "# LearnCholesky.scatter_samples_from_model(centers, precisions, dim1 = 4, dim2 = 5,  save_path=save_directory + 'samples.png')\n",
    "# LearnCholesky.plot_density_2d_marg(centers,factornet, dim1 = 4, dim2 = 5,save_path=save_directory + 'density.png')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
